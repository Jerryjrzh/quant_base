# æ•°æ®å¤„ç†æ¨¡å—æ–‡æ¡£

## ğŸ“Š æ¨¡å—æ¦‚è§ˆ

æ•°æ®å¤„ç†æ¨¡å—æ˜¯ç³»ç»Ÿçš„åŸºç¡€å±‚ï¼Œè´Ÿè´£è‚¡ç¥¨æ•°æ®çš„åŠ è½½ã€è§£æã€éªŒè¯å’Œç¼“å­˜ç®¡ç†ã€‚æ”¯æŒå¤šç§æ•°æ®æ ¼å¼ï¼Œæä¾›ç»Ÿä¸€çš„æ•°æ®è®¿é—®æ¥å£ã€‚

## ğŸ”§ æ ¸å¿ƒç»„ä»¶

### 1. DataLoader (data_loader.py)

#### ç±»ç»“æ„
```python
class DataLoader:
    def __init__(self, data_path: str = "data/vipdoc"):
        self.data_path = data_path
        self.cache = {}
        self.cache_size_limit = 100  # æœ€å¤§ç¼“å­˜è‚¡ç¥¨æ•°
        self.supported_formats = ['.day', '.lc5']
```

#### ä¸»è¦æ–¹æ³•

##### load_stock_data()
```python
def load_stock_data(self, symbol: str, period: str = "daily") -> pd.DataFrame:
    """
    åŠ è½½è‚¡ç¥¨æ•°æ®
    
    Args:
        symbol: è‚¡ç¥¨ä»£ç  (å¦‚ '000001', '600000')
        period: æ•°æ®å‘¨æœŸ ('daily', '5min')
    
    Returns:
        DataFrame with columns: [open, high, low, close, volume, amount]
        Index: DatetimeIndex
    
    Raises:
        FileNotFoundError: æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨
        ValueError: æ•°æ®æ ¼å¼é”™è¯¯
        DataQualityError: æ•°æ®è´¨é‡é—®é¢˜
    """
    
    # ç¼“å­˜æ£€æŸ¥
    cache_key = f"{symbol}_{period}"
    if cache_key in self.cache:
        logger.debug(f"ä»ç¼“å­˜åŠ è½½æ•°æ®: {symbol}")
        return self.cache[cache_key].copy()
    
    # ç¡®å®šæ–‡ä»¶è·¯å¾„
    file_path = self._get_file_path(symbol, period)
    
    # åŠ è½½æ•°æ®
    if period == "daily":
        df = self._load_daily_data(file_path)
    elif period == "5min":
        df = self._load_5min_data(file_path)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®å‘¨æœŸ: {period}")
    
    # æ•°æ®éªŒè¯å’Œæ¸…æ´—
    df = self._validate_and_clean_data(df, symbol)
    
    # æ›´æ–°ç¼“å­˜
    self._update_cache(cache_key, df)
    
    return df.copy()
```

##### _load_daily_data()
```python
def _load_daily_data(self, file_path: str) -> pd.DataFrame:
    """
    åŠ è½½æ—¥çº¿æ•°æ® (.dayæ ¼å¼)
    
    æ•°æ®æ ¼å¼:
    æ—¥æœŸ(YYYYMMDD) å¼€ç›˜ä»·*100 æœ€é«˜ä»·*100 æœ€ä½ä»·*100 æ”¶ç›˜ä»·*100 æˆäº¤é‡ æˆäº¤é¢
    """
    try:
        # è¯»å–åŸå§‹æ•°æ®
        df = pd.read_csv(
            file_path,
            sep='\t',
            header=None,
            names=['date', 'open', 'high', 'low', 'close', 'volume', 'amount'],
            dtype={
                'date': str,
                'open': int,
                'high': int,
                'low': int,
                'close': int,
                'volume': int,
                'amount': int
            }
        )
        
        # æ•°æ®è½¬æ¢
        df['date'] = pd.to_datetime(df['date'], format='%Y%m%d')
        df.set_index('date', inplace=True)
        
        # ä»·æ ¼æ•°æ®é™¤ä»¥100 (åŸå§‹æ•°æ®æ˜¯ä»·æ ¼*100)
        price_columns = ['open', 'high', 'low', 'close']
        df[price_columns] = df[price_columns] / 100.0
        
        # æŒ‰æ—¥æœŸæ’åº
        df.sort_index(inplace=True)
        
        return df
        
    except Exception as e:
        logger.error(f"åŠ è½½æ—¥çº¿æ•°æ®å¤±è´¥: {file_path}, é”™è¯¯: {str(e)}")
        raise DataLoadError(f"æ— æ³•åŠ è½½æ—¥çº¿æ•°æ®: {str(e)}")
```

##### _load_5min_data()
```python
def _load_5min_data(self, file_path: str) -> pd.DataFrame:
    """
    åŠ è½½5åˆ†é’Ÿæ•°æ® (.lc5æ ¼å¼)
    
    æ•°æ®æ ¼å¼:
    æ—¶é—´æˆ³ å¼€ç›˜ä»·*100 æœ€é«˜ä»·*100 æœ€ä½ä»·*100 æ”¶ç›˜ä»·*100 æˆäº¤é‡
    """
    try:
        # è¯»å–åŸå§‹æ•°æ®
        df = pd.read_csv(
            file_path,
            sep='\t',
            header=None,
            names=['timestamp', 'open', 'high', 'low', 'close', 'volume'],
            dtype={
                'timestamp': int,
                'open': int,
                'high': int,
                'low': int,
                'close': int,
                'volume': int
            }
        )
        
        # æ—¶é—´æˆ³è½¬æ¢ (å‡è®¾æ˜¯Unixæ—¶é—´æˆ³)
        df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')
        df.set_index('datetime', inplace=True)
        df.drop('timestamp', axis=1, inplace=True)
        
        # ä»·æ ¼æ•°æ®é™¤ä»¥100
        price_columns = ['open', 'high', 'low', 'close']
        df[price_columns] = df[price_columns] / 100.0
        
        # è¿‡æ»¤äº¤æ˜“æ—¶é—´ (9:30-11:30, 13:00-15:00)
        df = self._filter_trading_hours(df)
        
        return df
        
    except Exception as e:
        logger.error(f"åŠ è½½5åˆ†é’Ÿæ•°æ®å¤±è´¥: {file_path}, é”™è¯¯: {str(e)}")
        raise DataLoadError(f"æ— æ³•åŠ è½½5åˆ†é’Ÿæ•°æ®: {str(e)}")
```

##### _validate_and_clean_data()
```python
def _validate_and_clean_data(self, df: pd.DataFrame, symbol: str) -> pd.DataFrame:
    """
    æ•°æ®éªŒè¯å’Œæ¸…æ´—
    """
    if df.empty:
        raise DataQualityError(f"æ•°æ®ä¸ºç©º: {symbol}")
    
    # æ£€æŸ¥å¿…è¦åˆ—
    required_columns = ['open', 'high', 'low', 'close', 'volume']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        raise DataQualityError(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
    
    # æ£€æŸ¥ä»·æ ¼é€»è¾‘
    invalid_prices = (
        (df['high'] < df['low']) |
        (df['high'] < df['open']) |
        (df['high'] < df['close']) |
        (df['low'] > df['open']) |
        (df['low'] > df['close'])
    )
    
    if invalid_prices.any():
        logger.warning(f"{symbol}: å‘ç°{invalid_prices.sum()}æ¡ä»·æ ¼é€»è¾‘é”™è¯¯")
        # ä¿®æ­£æˆ–åˆ é™¤å¼‚å¸¸æ•°æ®
        df = df[~invalid_prices]
    
    # æ£€æŸ¥ç©ºå€¼
    null_counts = df.isnull().sum()
    if null_counts.any():
        logger.warning(f"{symbol}: å‘ç°ç©ºå€¼ {null_counts[null_counts > 0].to_dict()}")
        # å‰å‘å¡«å……
        df.fillna(method='ffill', inplace=True)
    
    # æ£€æŸ¥å¼‚å¸¸å€¼ (ä»·æ ¼å˜åŠ¨è¶…è¿‡20%å¯èƒ½å¼‚å¸¸)
    price_change = df['close'].pct_change().abs()
    outliers = price_change > 0.2
    if outliers.any():
        logger.warning(f"{symbol}: å‘ç°{outliers.sum()}ä¸ªä»·æ ¼å¼‚å¸¸å€¼")
    
    # æ£€æŸ¥æˆäº¤é‡å¼‚å¸¸ (æˆäº¤é‡ä¸º0æˆ–å¼‚å¸¸å¤§)
    volume_outliers = (df['volume'] == 0) | (df['volume'] > df['volume'].quantile(0.99) * 10)
    if volume_outliers.any():
        logger.warning(f"{symbol}: å‘ç°{volume_outliers.sum()}ä¸ªæˆäº¤é‡å¼‚å¸¸å€¼")
    
    return df
```

### 2. ç¼“å­˜ç®¡ç†ç³»ç»Ÿ

#### CacheManager
```python
class CacheManager:
    def __init__(self, max_size: int = 100, ttl: int = 3600):
        self.cache = {}
        self.access_times = {}
        self.max_size = max_size
        self.ttl = ttl  # ç¼“å­˜ç”Ÿå­˜æ—¶é—´(ç§’)
    
    def get(self, key: str) -> Optional[pd.DataFrame]:
        """è·å–ç¼“å­˜æ•°æ®"""
        if key not in self.cache:
            return None
        
        # æ£€æŸ¥TTL
        if time.time() - self.access_times[key] > self.ttl:
            self.remove(key)
            return None
        
        # æ›´æ–°è®¿é—®æ—¶é—´
        self.access_times[key] = time.time()
        return self.cache[key].copy()
    
    def put(self, key: str, data: pd.DataFrame):
        """å­˜å‚¨ç¼“å­˜æ•°æ®"""
        # æ£€æŸ¥ç¼“å­˜å¤§å°é™åˆ¶
        if len(self.cache) >= self.max_size:
            self._evict_lru()
        
        self.cache[key] = data.copy()
        self.access_times[key] = time.time()
    
    def _evict_lru(self):
        """LRUæ·˜æ±°ç­–ç•¥"""
        if not self.access_times:
            return
        
        # æ‰¾åˆ°æœ€ä¹…æœªè®¿é—®çš„key
        lru_key = min(self.access_times.keys(), 
                     key=lambda k: self.access_times[k])
        self.remove(lru_key)
    
    def remove(self, key: str):
        """ç§»é™¤ç¼“å­˜é¡¹"""
        self.cache.pop(key, None)
        self.access_times.pop(key, None)
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache.clear()
        self.access_times.clear()
    
    def get_stats(self) -> dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_memory = sum(
            df.memory_usage(deep=True).sum() 
            for df in self.cache.values()
        )
        
        return {
            'cache_size': len(self.cache),
            'max_size': self.max_size,
            'memory_usage_mb': total_memory / 1024 / 1024,
            'hit_rate': getattr(self, '_hit_count', 0) / max(getattr(self, '_total_requests', 1), 1)
        }
```

### 3. æ•°æ®è´¨é‡ç›‘æ§

#### DataQualityMonitor
```python
class DataQualityMonitor:
    def __init__(self):
        self.quality_reports = {}
    
    def check_data_quality(self, df: pd.DataFrame, symbol: str) -> dict:
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        report = {
            'symbol': symbol,
            'total_records': len(df),
            'date_range': {
                'start': df.index.min().strftime('%Y-%m-%d'),
                'end': df.index.max().strftime('%Y-%m-%d')
            },
            'issues': []
        }
        
        # æ£€æŸ¥æ•°æ®è¿ç»­æ€§
        date_gaps = self._check_date_continuity(df)
        if date_gaps:
            report['issues'].append({
                'type': 'date_gaps',
                'count': len(date_gaps),
                'details': date_gaps[:5]  # åªæ˜¾ç¤ºå‰5ä¸ª
            })
        
        # æ£€æŸ¥ä»·æ ¼å¼‚å¸¸
        price_anomalies = self._check_price_anomalies(df)
        if price_anomalies:
            report['issues'].append({
                'type': 'price_anomalies',
                'count': len(price_anomalies),
                'details': price_anomalies[:5]
            })
        
        # æ£€æŸ¥æˆäº¤é‡å¼‚å¸¸
        volume_anomalies = self._check_volume_anomalies(df)
        if volume_anomalies:
            report['issues'].append({
                'type': 'volume_anomalies',
                'count': len(volume_anomalies),
                'details': volume_anomalies[:5]
            })
        
        # è®¡ç®—è´¨é‡åˆ†æ•°
        report['quality_score'] = self._calculate_quality_score(report)
        
        self.quality_reports[symbol] = report
        return report
    
    def _check_date_continuity(self, df: pd.DataFrame) -> list:
        """æ£€æŸ¥æ—¥æœŸè¿ç»­æ€§"""
        gaps = []
        dates = df.index.to_series()
        
        for i in range(1, len(dates)):
            current_date = dates.iloc[i]
            prev_date = dates.iloc[i-1]
            
            # è®¡ç®—å·¥ä½œæ—¥å·®å¼‚
            expected_next = prev_date + pd.Timedelta(days=1)
            while expected_next.weekday() >= 5:  # è·³è¿‡å‘¨æœ«
                expected_next += pd.Timedelta(days=1)
            
            if current_date > expected_next:
                gaps.append({
                    'start': prev_date.strftime('%Y-%m-%d'),
                    'end': current_date.strftime('%Y-%m-%d'),
                    'days': (current_date - prev_date).days
                })
        
        return gaps
    
    def _check_price_anomalies(self, df: pd.DataFrame) -> list:
        """æ£€æŸ¥ä»·æ ¼å¼‚å¸¸"""
        anomalies = []
        
        # æ£€æŸ¥ä»·æ ¼é€»è¾‘é”™è¯¯
        logic_errors = (
            (df['high'] < df['low']) |
            (df['high'] < df['open']) |
            (df['high'] < df['close']) |
            (df['low'] > df['open']) |
            (df['low'] > df['close'])
        )
        
        for date in df[logic_errors].index:
            anomalies.append({
                'date': date.strftime('%Y-%m-%d'),
                'type': 'logic_error',
                'data': df.loc[date].to_dict()
            })
        
        # æ£€æŸ¥å¼‚å¸¸æ³¢åŠ¨
        price_change = df['close'].pct_change().abs()
        extreme_changes = price_change > 0.2  # å•æ—¥æ¶¨è·Œè¶…è¿‡20%
        
        for date in df[extreme_changes].index:
            if pd.isna(price_change.loc[date]):
                continue
            anomalies.append({
                'date': date.strftime('%Y-%m-%d'),
                'type': 'extreme_change',
                'change_pct': price_change.loc[date] * 100
            })
        
        return anomalies
    
    def _check_volume_anomalies(self, df: pd.DataFrame) -> list:
        """æ£€æŸ¥æˆäº¤é‡å¼‚å¸¸"""
        anomalies = []
        
        # é›¶æˆäº¤é‡
        zero_volume = df['volume'] == 0
        for date in df[zero_volume].index:
            anomalies.append({
                'date': date.strftime('%Y-%m-%d'),
                'type': 'zero_volume'
            })
        
        # å¼‚å¸¸å¤§æˆäº¤é‡
        volume_threshold = df['volume'].quantile(0.99) * 5
        extreme_volume = df['volume'] > volume_threshold
        
        for date in df[extreme_volume].index:
            anomalies.append({
                'date': date.strftime('%Y-%m-%d'),
                'type': 'extreme_volume',
                'volume': int(df.loc[date, 'volume'])
            })
        
        return anomalies
    
    def _calculate_quality_score(self, report: dict) -> float:
        """è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•° (0-100)"""
        base_score = 100
        
        for issue in report['issues']:
            if issue['type'] == 'date_gaps':
                base_score -= min(issue['count'] * 2, 20)
            elif issue['type'] == 'price_anomalies':
                base_score -= min(issue['count'] * 1, 15)
            elif issue['type'] == 'volume_anomalies':
                base_score -= min(issue['count'] * 0.5, 10)
        
        return max(base_score, 0)
```

### 4. æ•°æ®é¢„å¤„ç†å·¥å…·

#### DataPreprocessor
```python
class DataPreprocessor:
    def __init__(self):
        self.processors = {
            'fill_missing': self._fill_missing_values,
            'remove_outliers': self._remove_outliers,
            'smooth_prices': self._smooth_prices,
            'adjust_splits': self._adjust_for_splits
        }
    
    def preprocess(self, df: pd.DataFrame, operations: list) -> pd.DataFrame:
        """æ‰§è¡Œæ•°æ®é¢„å¤„ç†"""
        result_df = df.copy()
        
        for operation in operations:
            if operation in self.processors:
                result_df = self.processors[operation](result_df)
                logger.debug(f"æ‰§è¡Œé¢„å¤„ç†æ“ä½œ: {operation}")
            else:
                logger.warning(f"æœªçŸ¥çš„é¢„å¤„ç†æ“ä½œ: {operation}")
        
        return result_df
    
    def _fill_missing_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """å¡«å……ç¼ºå¤±å€¼"""
        # ä»·æ ¼æ•°æ®ä½¿ç”¨å‰å‘å¡«å……
        price_columns = ['open', 'high', 'low', 'close']
        df[price_columns] = df[price_columns].fillna(method='ffill')
        
        # æˆäº¤é‡ä½¿ç”¨0å¡«å……
        df['volume'] = df['volume'].fillna(0)
        
        return df
    
    def _remove_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç§»é™¤å¼‚å¸¸å€¼"""
        # ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹ä»·æ ¼å¼‚å¸¸å€¼
        for col in ['open', 'high', 'low', 'close']:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            # æ ‡è®°å¼‚å¸¸å€¼ä½†ä¸åˆ é™¤ï¼Œè€Œæ˜¯ç”¨è¾¹ç•Œå€¼æ›¿æ¢
            df.loc[df[col] < lower_bound, col] = lower_bound
            df.loc[df[col] > upper_bound, col] = upper_bound
        
        return df
    
    def _smooth_prices(self, df: pd.DataFrame) -> pd.DataFrame:
        """ä»·æ ¼å¹³æ»‘å¤„ç†"""
        # ä½¿ç”¨3æ—¥ç§»åŠ¨å¹³å‡å¹³æ»‘ä»·æ ¼
        window = 3
        
        for col in ['open', 'high', 'low', 'close']:
            df[f'{col}_smoothed'] = df[col].rolling(window=window, center=True).mean()
            # ç”¨å¹³æ»‘åçš„å€¼æ›¿æ¢åŸå€¼
            df[col] = df[f'{col}_smoothed'].fillna(df[col])
            df.drop(f'{col}_smoothed', axis=1, inplace=True)
        
        return df
    
    def _adjust_for_splits(self, df: pd.DataFrame) -> pd.DataFrame:
        """è‚¡ç¥¨åˆ†å‰²è°ƒæ•´"""
        # æ£€æµ‹å¯èƒ½çš„è‚¡ç¥¨åˆ†å‰² (ä»·æ ¼çªç„¶å‡åŠ)
        price_ratio = df['close'] / df['close'].shift(1)
        
        # æ£€æµ‹åˆ†å‰²ç‚¹ (ä»·æ ¼æ¯”ç‡åœ¨0.4-0.6ä¹‹é—´ï¼Œå¯èƒ½æ˜¯1:2åˆ†å‰²)
        split_points = (price_ratio > 0.4) & (price_ratio < 0.6)
        
        if split_points.any():
            logger.info(f"æ£€æµ‹åˆ°å¯èƒ½çš„è‚¡ç¥¨åˆ†å‰²ç‚¹: {split_points.sum()}ä¸ª")
            
            # å¯¹åˆ†å‰²å‰çš„æ•°æ®è¿›è¡Œè°ƒæ•´
            for split_date in df[split_points].index:
                split_ratio = price_ratio.loc[split_date]
                
                # è°ƒæ•´åˆ†å‰²å‰çš„ä»·æ ¼æ•°æ®
                before_split = df.index < split_date
                price_columns = ['open', 'high', 'low', 'close']
                df.loc[before_split, price_columns] *= split_ratio
                
                # è°ƒæ•´æˆäº¤é‡ (åˆ†å‰²åæˆäº¤é‡åº”è¯¥å¢åŠ )
                df.loc[before_split, 'volume'] /= split_ratio
        
        return df
```

## ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

### åŸºæœ¬æ•°æ®åŠ è½½
```python
from backend.data_loader import DataLoader

# åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
loader = DataLoader("data/vipdoc")

# åŠ è½½æ—¥çº¿æ•°æ®
daily_data = loader.load_stock_data("000001", "daily")
print(f"åŠ è½½äº† {len(daily_data)} æ¡æ—¥çº¿æ•°æ®")

# åŠ è½½5åˆ†é’Ÿæ•°æ®
min5_data = loader.load_stock_data("000001", "5min")
print(f"åŠ è½½äº† {len(min5_data)} æ¡5åˆ†é’Ÿæ•°æ®")

# è·å–è‚¡ç¥¨åˆ—è¡¨
stock_list = loader.get_stock_list()
print(f"å…±æœ‰ {len(stock_list)} åªè‚¡ç¥¨")
```

### æ•°æ®è´¨é‡æ£€æŸ¥
```python
from backend.data_loader import DataQualityMonitor

# åˆå§‹åŒ–è´¨é‡ç›‘æ§å™¨
monitor = DataQualityMonitor()

# æ£€æŸ¥æ•°æ®è´¨é‡
quality_report = monitor.check_data_quality(daily_data, "000001")

print(f"æ•°æ®è´¨é‡åˆ†æ•°: {quality_report['quality_score']}")
print(f"å‘ç°é—®é¢˜: {len(quality_report['issues'])} ä¸ª")

for issue in quality_report['issues']:
    print(f"- {issue['type']}: {issue['count']} ä¸ª")
```

### æ•°æ®é¢„å¤„ç†
```python
from backend.data_loader import DataPreprocessor

# åˆå§‹åŒ–é¢„å¤„ç†å™¨
preprocessor = DataPreprocessor()

# æ‰§è¡Œé¢„å¤„ç†
processed_data = preprocessor.preprocess(
    daily_data, 
    ['fill_missing', 'remove_outliers', 'smooth_prices']
)

print("æ•°æ®é¢„å¤„ç†å®Œæˆ")
```

### æ‰¹é‡æ•°æ®å¤„ç†
```python
def batch_load_and_process():
    """æ‰¹é‡åŠ è½½å’Œå¤„ç†æ•°æ®"""
    loader = DataLoader()
    monitor = DataQualityMonitor()
    preprocessor = DataPreprocessor()
    
    stock_list = loader.get_stock_list()[:10]  # å¤„ç†å‰10åªè‚¡ç¥¨
    
    results = {}
    
    for symbol in stock_list:
        try:
            # åŠ è½½æ•°æ®
            df = loader.load_stock_data(symbol)
            
            # è´¨é‡æ£€æŸ¥
            quality_report = monitor.check_data_quality(df, symbol)
            
            # å¦‚æœè´¨é‡åˆ†æ•°å¤ªä½ï¼Œè·³è¿‡
            if quality_report['quality_score'] < 70:
                logger.warning(f"{symbol}: æ•°æ®è´¨é‡è¿‡ä½ï¼Œè·³è¿‡å¤„ç†")
                continue
            
            # é¢„å¤„ç†
            processed_df = preprocessor.preprocess(df, ['fill_missing', 'remove_outliers'])
            
            results[symbol] = {
                'data': processed_df,
                'quality_score': quality_report['quality_score'],
                'record_count': len(processed_df)
            }
            
            logger.info(f"{symbol}: å¤„ç†å®Œæˆï¼Œè´¨é‡åˆ†æ•° {quality_report['quality_score']}")
            
        except Exception as e:
            logger.error(f"{symbol}: å¤„ç†å¤±è´¥ - {str(e)}")
    
    return results

# æ‰§è¡Œæ‰¹é‡å¤„ç†
batch_results = batch_load_and_process()
print(f"æˆåŠŸå¤„ç† {len(batch_results)} åªè‚¡ç¥¨")
```

## ğŸ¯ æ€§èƒ½ä¼˜åŒ–

### 1. å¹¶è¡Œæ•°æ®åŠ è½½
```python
import concurrent.futures
from typing import List, Dict

def parallel_load_stocks(symbols: List[str], max_workers: int = 4) -> Dict[str, pd.DataFrame]:
    """å¹¶è¡ŒåŠ è½½å¤šåªè‚¡ç¥¨æ•°æ®"""
    loader = DataLoader()
    results = {}
    
    def load_single_stock(symbol):
        try:
            return symbol, loader.load_stock_data(symbol)
        except Exception as e:
            logger.error(f"åŠ è½½ {symbol} å¤±è´¥: {str(e)}")
            return symbol, None
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_symbol = {
            executor.submit(load_single_stock, symbol): symbol 
            for symbol in symbols
        }
        
        for future in concurrent.futures.as_completed(future_to_symbol):
            symbol, data = future.result()
            if data is not None:
                results[symbol] = data
    
    return results
```

### 2. å†…å­˜ä¼˜åŒ–
```python
def optimize_dataframe_memory(df: pd.DataFrame) -> pd.DataFrame:
    """ä¼˜åŒ–DataFrameå†…å­˜ä½¿ç”¨"""
    # è½¬æ¢æ•°æ®ç±»å‹ä»¥èŠ‚çœå†…å­˜
    for col in df.columns:
        if df[col].dtype == 'int64':
            if df[col].min() >= 0:
                if df[col].max() < 2**16:
                    df[col] = df[col].astype('uint16')
                elif df[col].max() < 2**32:
                    df[col] = df[col].astype('uint32')
            else:
                if df[col].min() >= -2**15 and df[col].max() < 2**15:
                    df[col] = df[col].astype('int16')
                elif df[col].min() >= -2**31 and df[col].max() < 2**31:
                    df[col] = df[col].astype('int32')
        
        elif df[col].dtype == 'float64':
            df[col] = df[col].astype('float32')
    
    return df
```

æ•°æ®å¤„ç†æ¨¡å—ä¸ºæ•´ä¸ªç³»ç»Ÿæä¾›äº†å¯é ã€é«˜æ•ˆçš„æ•°æ®åŸºç¡€ï¼Œé€šè¿‡å®Œå–„çš„ç¼“å­˜æœºåˆ¶ã€è´¨é‡ç›‘æ§å’Œé¢„å¤„ç†åŠŸèƒ½ï¼Œç¡®ä¿äº†æ•°æ®çš„å‡†ç¡®æ€§å’Œç³»ç»Ÿçš„æ€§èƒ½ã€‚