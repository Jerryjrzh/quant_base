"""
é«˜æ€§èƒ½ä¼˜åŒ–æ¨¡å— - æä¾›å¤šçº¿ç¨‹å’Œç¼“å­˜ä¼˜åŒ–
é€‚ç”¨äºé«˜æ€§èƒ½å¤„ç†å™¨å’Œå¤§å†…å­˜ç¯å¢ƒ
"""

import os
import json
import time
import threading
import glob
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from typing import Dict, List, Callable, Any, Optional, Tuple, Union
import multiprocessing
from functools import lru_cache
import hashlib
import pickle
import gc
import queue
import sys

# å…¨å±€æ€§èƒ½é…ç½®
PERFORMANCE_CONFIG = {
    'max_memory_usage_percent': 85,  # æœ€å¤§å†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯”
    'aggressive_gc': True,           # æ˜¯å¦å¯ç”¨æ¿€è¿›çš„åƒåœ¾å›æ”¶
    'use_mmap_for_large_data': False, # å¯¹å¤§æ•°æ®ä½¿ç”¨å†…å­˜æ˜ å°„
    'large_data_threshold_mb': 100,  # å¤§æ•°æ®é˜ˆå€¼(MB)
    'parallel_io': True,             # å¹¶è¡ŒIOæ“ä½œ
    'io_threads': 8,                 # IOçº¿ç¨‹æ•°
    'optimize_numpy': False,         # ä¼˜åŒ–NumPyæ“ä½œ
    'cache_compression': False,      # ç¼“å­˜å‹ç¼©(é™ä½å†…å­˜å ç”¨ä½†å¢åŠ CPUä½¿ç”¨)
    'prefetch_data': True,           # æ•°æ®é¢„å–
    'adaptive_batch_size': True,     # è‡ªé€‚åº”æ‰¹å¤„ç†å¤§å°
    'profile_hotspots': False        # æ€§èƒ½çƒ­ç‚¹åˆ†æ
}

class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ªå™¨ - æ˜¾ç¤ºå¤šçº¿ç¨‹ä»»åŠ¡è¿›åº¦"""
    
    def __init__(self, total: int, task_name: str = "ä»»åŠ¡", update_interval: int = 5):
        self.total = total
        self.completed = 0
        self.task_name = task_name
        self.start_time = time.time()
        self.update_interval = update_interval  # æ›´æ–°é—´éš”ï¼ˆç§’ï¼‰
        self.last_update_time = self.start_time
        self.lock = threading.Lock()
        
        # é€Ÿåº¦ä¼°è®¡
        self.speed_samples = []
        self.last_completed = 0
        self.last_sample_time = self.start_time
    
    def update(self, increment: int = 1) -> None:
        """æ›´æ–°è¿›åº¦"""
        with self.lock:
            self.completed += increment
            current_time = time.time()
            
            # æ›´æ–°é€Ÿåº¦æ ·æœ¬
            if current_time - self.last_sample_time >= 1.0:  # æ¯ç§’é‡‡æ ·ä¸€æ¬¡
                time_diff = current_time - self.last_sample_time
                completed_diff = self.completed - self.last_completed
                if time_diff > 0:
                    speed = completed_diff / time_diff
                    self.speed_samples.append(speed)
                    # ä¿ç•™æœ€è¿‘10ä¸ªæ ·æœ¬
                    if len(self.speed_samples) > 10:
                        self.speed_samples = self.speed_samples[-10:]
                
                self.last_sample_time = current_time
                self.last_completed = self.completed
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ›´æ–°æ˜¾ç¤º
            if (self.completed == self.total or 
                self.completed == 1 or 
                current_time - self.last_update_time >= self.update_interval):
                
                self.display_progress()
                self.last_update_time = current_time
    
    def display_progress(self) -> None:
        """æ˜¾ç¤ºå½“å‰è¿›åº¦"""
        elapsed = time.time() - self.start_time
        progress = self.completed / self.total
        
        # è®¡ç®—å½“å‰é€Ÿåº¦ï¼ˆæ¯ç§’å®Œæˆçš„ä»»åŠ¡æ•°ï¼‰
        if self.speed_samples:
            current_speed = sum(self.speed_samples) / len(self.speed_samples)
        else:
            current_speed = self.completed / elapsed if elapsed > 0 else 0
        
        # ä¼°è®¡å‰©ä½™æ—¶é—´
        if current_speed > 0:
            remaining = (self.total - self.completed) / current_speed
        else:
            remaining = 0
        
        # æ ¼å¼åŒ–æ—¶é—´
        def format_time(seconds):
            if seconds < 60:
                return f"{seconds:.1f}ç§’"
            elif seconds < 3600:
                return f"{seconds/60:.1f}åˆ†é’Ÿ"
            else:
                return f"{seconds/3600:.1f}å°æ—¶"
        
        # æ ¼å¼åŒ–é€Ÿåº¦
        speed_str = f"{current_speed:.1f}ä»»åŠ¡/ç§’" if current_speed >= 0.1 else f"{current_speed*60:.1f}ä»»åŠ¡/åˆ†é’Ÿ"
        
        print(f"â³ {self.task_name}è¿›åº¦: {progress*100:.1f}% ({self.completed}/{self.total}) "
              f"é€Ÿåº¦: {speed_str}, "
              f"å·²ç”¨æ—¶é—´: {format_time(elapsed)}, "
              f"é¢„è®¡å‰©ä½™: {format_time(remaining)}")

class BatchProcessor:
    """æ‰¹é‡å¤„ç†å™¨ - é«˜æ€§èƒ½å¤„ç†å¤§é‡ä»»åŠ¡"""
    
    def __init__(self, max_workers: Optional[int] = None, use_process_pool: bool = False):
        """
        åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨
        
        Args:
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹/è¿›ç¨‹æ•°ï¼Œé»˜è®¤è‡ªåŠ¨æ ¹æ®ç³»ç»Ÿèµ„æºç¡®å®š
            use_process_pool: æ˜¯å¦ä½¿ç”¨è¿›ç¨‹æ± è€Œéçº¿ç¨‹æ± ï¼ˆé€‚ç”¨äºCPUå¯†é›†å‹ä»»åŠ¡ï¼‰
        """
        # è‡ªåŠ¨ç¡®å®šæœ€ä½³çº¿ç¨‹æ•°
        if max_workers is None:
            max_workers = min(multiprocessing.cpu_count() * 2, 32)
        
        self.max_workers = max_workers
        self.use_process_pool = use_process_pool
        
        print(f"ğŸ§µ æ‰¹å¤„ç†å™¨åˆå§‹åŒ–: {'è¿›ç¨‹æ± ' if use_process_pool else 'çº¿ç¨‹æ± '}, {max_workers} ä¸ªå·¥ä½œå•å…ƒ")
    
    def process_stocks_batch(self, 
                            stock_codes: List[str], 
                            process_func: Callable[[str], Any],
                            batch_size: Optional[int] = None) -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†è‚¡ç¥¨
        
        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            process_func: å¤„ç†å•åªè‚¡ç¥¨çš„å‡½æ•°ï¼Œæ¥å—è‚¡ç¥¨ä»£ç ï¼Œè¿”å›å¤„ç†ç»“æœ
            batch_size: æ¯æ‰¹å¤„ç†çš„è‚¡ç¥¨æ•°é‡ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨ç¡®å®š
        
        Returns:
            Dict[str, Any]: è‚¡ç¥¨ä»£ç åˆ°å¤„ç†ç»“æœçš„æ˜ å°„
        """
        # è‡ªåŠ¨ç¡®å®šæ‰¹å¤„ç†å¤§å°
        if batch_size is None:
            # æ ¹æ®è‚¡ç¥¨æ•°é‡å’Œå·¥ä½œçº¿ç¨‹æ•°åŠ¨æ€è°ƒæ•´
            if len(stock_codes) > 1000:
                batch_size = min(100, len(stock_codes) // (self.max_workers * 2))
            elif len(stock_codes) > 100:
                batch_size = min(20, len(stock_codes) // self.max_workers)
            else:
                batch_size = max(1, min(10, len(stock_codes) // 2))
        
        results = {}
        total_stocks = len(stock_codes)
        
        # åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨
        progress = ProgressTracker(total_stocks, "æ‰¹é‡å¤„ç†")
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, total_stocks, batch_size):
            batch = stock_codes[i:i+batch_size]
            batch_results = self._process_batch(batch, process_func, progress)
            results.update(batch_results)
            
            # æ˜¾ç¤ºæ‰¹æ¬¡å®Œæˆæƒ…å†µ
            print(f"âœ… å®Œæˆæ‰¹æ¬¡ {i//batch_size + 1}/{(total_stocks + batch_size - 1)//batch_size}")
            
            # æ¯å¤„ç†5ä¸ªæ‰¹æ¬¡ï¼Œæ‰§è¡Œä¸€æ¬¡åƒåœ¾å›æ”¶
            if (i // batch_size) % 5 == 4:
                gc.collect()
        
        return results
    
    def _process_batch(self, 
                      batch: List[str], 
                      process_func: Callable[[str], Any],
                      progress: ProgressTracker) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡ - æ”¯æŒçº¿ç¨‹æ± æˆ–è¿›ç¨‹æ± """
        batch_results = {}
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹é€‰æ‹©åˆé€‚çš„æ‰§è¡Œå™¨
        if self.use_process_pool:
            # ä½¿ç”¨è¿›ç¨‹æ±  - é€‚åˆCPUå¯†é›†å‹ä»»åŠ¡
            with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_stock = {
                    executor.submit(self._process_with_progress, stock_code, process_func, progress): stock_code
                    for stock_code in batch
                }
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_stock):
                    stock_code = future_to_stock[future]
                    try:
                        result = future.result()
                        batch_results[stock_code] = result
                    except Exception as e:
                        print(f"âŒ å¤„ç† {stock_code} æ—¶å‡ºé”™: {e}")
                        batch_results[stock_code] = {'error': f'å¤„ç†å¼‚å¸¸: {e}'}
        else:
            # ä½¿ç”¨çº¿ç¨‹æ±  - é€‚åˆIOå¯†é›†å‹ä»»åŠ¡
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_stock = {
                    executor.submit(self._process_with_progress, stock_code, process_func, progress): stock_code
                    for stock_code in batch
                }
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_stock):
                    stock_code = future_to_stock[future]
                    try:
                        result = future.result()
                        batch_results[stock_code] = result
                    except Exception as e:
                        print(f"âŒ å¤„ç† {stock_code} æ—¶å‡ºé”™: {e}")
                        batch_results[stock_code] = {'error': f'å¤„ç†å¼‚å¸¸: {e}'}
        
        return batch_results
    
    def _process_with_progress(self, 
                              stock_code: str, 
                              process_func: Callable[[str], Any],
                              progress: ProgressTracker) -> Any:
        """å¤„ç†å•åªè‚¡ç¥¨å¹¶æ›´æ–°è¿›åº¦"""
        try:
            result = process_func(stock_code)
            return result
        finally:
            # æ— è®ºæˆåŠŸå¤±è´¥éƒ½æ›´æ–°è¿›åº¦
            progress.update()

class SmartCache:
    """æ™ºèƒ½ç¼“å­˜ - ç¼“å­˜è®¡ç®—ç»“æœä»¥æé«˜æ€§èƒ½"""
    
    def __init__(self, cache_dir: str = "analysis_cache", max_memory_items: int = 10000):
        """
        åˆå§‹åŒ–æ™ºèƒ½ç¼“å­˜
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•
            max_memory_items: å†…å­˜ä¸­æœ€å¤šä¿å­˜çš„ç¼“å­˜é¡¹æ•°é‡
        """
        self.cache_dir = cache_dir
        self.binary_cache_dir = os.path.join(cache_dir, "binary")
        os.makedirs(cache_dir, exist_ok=True)
        os.makedirs(self.binary_cache_dir, exist_ok=True)
        
        # ä½¿ç”¨LRUç¼“å­˜ç­–ç•¥çš„å†…å­˜ç¼“å­˜
        self.memory_cache = {}
        self.max_memory_items = max_memory_items
        self.access_count = {}
        self.lock = threading.Lock()
        
        # ç¼“å­˜ç»Ÿè®¡
        self.stats = {
            'hits': 0,
            'misses': 0,
            'memory_hits': 0,
            'disk_hits': 0
        }
        
        # é¢„åŠ è½½å¸¸ç”¨ç¼“å­˜
        self._preload_common_cache()
    
    def _preload_common_cache(self):
        """é¢„åŠ è½½å¸¸ç”¨ç¼“å­˜é¡¹åˆ°å†…å­˜"""
        try:
            # æŸ¥æ‰¾æœ€è¿‘ä¿®æ”¹çš„ç¼“å­˜æ–‡ä»¶
            json_files = glob.glob(os.path.join(self.cache_dir, "*.json"))
            binary_files = glob.glob(os.path.join(self.binary_cache_dir, "*.pkl"))
            
            all_files = [(f, os.path.getmtime(f)) for f in json_files + binary_files]
            all_files.sort(key=lambda x: x[1], reverse=True)  # æŒ‰ä¿®æ”¹æ—¶é—´é™åºæ’åº
            
            # é¢„åŠ è½½æœ€è¿‘çš„100ä¸ªæ–‡ä»¶
            preload_count = min(50, len(all_files))
            if preload_count > 0:
              # print(f"ğŸ”„ é¢„åŠ è½½ {preload_count} ä¸ªå¸¸ç”¨ç¼“å­˜é¡¹...")
                
                for i, (file_path, _) in enumerate(all_files[:preload_count]):
                    try:
                        if file_path.endswith('.json'):
                            key = os.path.basename(file_path)[:-5]  # å»æ‰.jsonåç¼€
                            with open(file_path, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                        else:  # .pklæ–‡ä»¶
                            key = os.path.basename(file_path)[:-4]  # å»æ‰.pklåç¼€
                            with open(file_path, 'rb') as f:
                                data = pickle.load(f)
                        
                        # æ·»åŠ åˆ°å†…å­˜ç¼“å­˜
                        with self.lock:
                            self.memory_cache[key] = data
                            self.access_count[key] = 1
                    except:
                        pass
                
               # print(f"âœ… ç¼“å­˜é¢„åŠ è½½å®Œæˆï¼Œå·²åŠ è½½ {len(self.memory_cache)} é¡¹")
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜é¢„åŠ è½½å¤±è´¥: {e}")
    
    def get(self, key: str, max_age_hours: int = 24) -> Optional[Any]:
        """
        è·å–ç¼“å­˜æ•°æ®
        
        Args:
            key: ç¼“å­˜é”®
            max_age_hours: æœ€å¤§ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆå°æ—¶ï¼‰
        
        Returns:
            ç¼“å­˜æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨æˆ–è¿‡æœŸåˆ™è¿”å›None
        """
        # å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
        with self.lock:
            if key in self.memory_cache:
                self.stats['hits'] += 1
                self.stats['memory_hits'] += 1
                self.access_count[key] = self.access_count.get(key, 0) + 1
                return self.memory_cache[key]
        
        # æ£€æŸ¥äºŒè¿›åˆ¶ç¼“å­˜ï¼ˆæ›´å¿«ï¼‰
        binary_path = os.path.join(self.binary_cache_dir, f"{key}.pkl")
        if os.path.exists(binary_path):
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¿‡æœŸ
            file_age_hours = (datetime.now() - datetime.fromtimestamp(os.path.getmtime(binary_path))).total_seconds() / 3600
            if file_age_hours <= max_age_hours:
                try:
                    with open(binary_path, 'rb') as f:
                        data = pickle.load(f)
                    
                    # æ›´æ–°å†…å­˜ç¼“å­˜
                    self._update_memory_cache(key, data)
                    
                    self.stats['hits'] += 1
                    self.stats['disk_hits'] += 1
                    return data
                except:
                    pass
        
        # æ£€æŸ¥JSONç¼“å­˜ï¼ˆå…¼å®¹æ€§æ›´å¥½ï¼‰
        json_path = os.path.join(self.cache_dir, f"{key}.json")
        if os.path.exists(json_path):
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¿‡æœŸ
            file_age_hours = (datetime.now() - datetime.fromtimestamp(os.path.getmtime(json_path))).total_seconds() / 3600
            if file_age_hours <= max_age_hours:
                try:
                    with open(json_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    
                    # æ›´æ–°å†…å­˜ç¼“å­˜
                    self._update_memory_cache(key, data)
                    
                    # åŒæ—¶æ›´æ–°äºŒè¿›åˆ¶ç¼“å­˜ï¼ˆåŠ é€Ÿæœªæ¥çš„è®¿é—®ï¼‰
                    try:
                        with open(binary_path, 'wb') as f:
                            pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
                    except:
                        pass
                    
                    self.stats['hits'] += 1
                    self.stats['disk_hits'] += 1
                    return data
                except:
                    pass
        
        self.stats['misses'] += 1
        return None
    
    def _update_memory_cache(self, key: str, data: Any) -> None:
        """æ›´æ–°å†…å­˜ç¼“å­˜ï¼Œå¦‚æœè¶…å‡ºå®¹é‡åˆ™ç§»é™¤æœ€å°‘è®¿é—®çš„é¡¹"""
        with self.lock:
            # å¦‚æœå†…å­˜ç¼“å­˜å·²æ»¡ï¼Œç§»é™¤æœ€å°‘è®¿é—®çš„é¡¹
            if len(self.memory_cache) >= self.max_memory_items and key not in self.memory_cache:
                # æŒ‰è®¿é—®æ¬¡æ•°æ’åº
                sorted_items = sorted(self.access_count.items(), key=lambda x: x[1])
                # ç§»é™¤å‰10%æœ€å°‘è®¿é—®çš„é¡¹
                items_to_remove = sorted_items[:max(1, len(sorted_items) // 10)]
                for k, _ in items_to_remove:
                    if k in self.memory_cache:
                        del self.memory_cache[k]
                        del self.access_count[k]
            
            # æ·»åŠ æ–°é¡¹
            self.memory_cache[key] = data
            self.access_count[key] = self.access_count.get(key, 0) + 1
    
    def set(self, key: str, data: Any, use_binary: bool = True) -> None:
        """
        è®¾ç½®ç¼“å­˜æ•°æ®
        
        Args:
            key: ç¼“å­˜é”®
            data: è¦ç¼“å­˜çš„æ•°æ®
            use_binary: æ˜¯å¦ä½¿ç”¨äºŒè¿›åˆ¶æ ¼å¼ï¼ˆæ›´å¿«ä½†å…¼å®¹æ€§è¾ƒå·®ï¼‰
        """
        # æ›´æ–°å†…å­˜ç¼“å­˜
        self._update_memory_cache(key, data)
        
        # æ›´æ–°æ–‡ä»¶ç¼“å­˜
        json_path = os.path.join(self.cache_dir, f"{key}.json")
        binary_path = os.path.join(self.binary_cache_dir, f"{key}.pkl")
        
        try:
            # æ€»æ˜¯ä¿å­˜JSONæ ¼å¼ï¼ˆå…¼å®¹æ€§å¥½ï¼‰
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            # å¦‚æœéœ€è¦ï¼Œä¹Ÿä¿å­˜äºŒè¿›åˆ¶æ ¼å¼ï¼ˆé€Ÿåº¦å¿«ï¼‰
            if use_binary:
                try:
                    with open(binary_path, 'wb') as f:
                        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
                except:
                    pass
        except Exception as e:
            print(f"âŒ ç¼“å­˜å†™å…¥å¤±è´¥: {e}")
    
    def clear(self, older_than_hours: Optional[int] = None) -> int:
        """
        æ¸…ç†ç¼“å­˜
        
        Args:
            older_than_hours: æ¸…ç†æŒ‡å®šå°æ—¶æ•°ä»¥å‰çš„ç¼“å­˜ï¼ŒNoneè¡¨ç¤ºæ¸…ç†æ‰€æœ‰
        
        Returns:
            æ¸…ç†çš„ç¼“å­˜æ•°é‡
        """
        cleared = 0
        
        # æ¸…ç†å†…å­˜ç¼“å­˜
        with self.lock:
            self.memory_cache.clear()
            self.access_count.clear()
        
        # æ¸…ç†æ–‡ä»¶ç¼“å­˜
        if older_than_hours is not None:
            cutoff_time = datetime.now() - timedelta(hours=older_than_hours)
            
            # æ¸…ç†JSONç¼“å­˜
            for file_name in os.listdir(self.cache_dir):
                if file_name.endswith('.json'):
                    file_path = os.path.join(self.cache_dir, file_name)
                    if datetime.fromtimestamp(os.path.getmtime(file_path)) < cutoff_time:
                        try:
                            os.remove(file_path)
                            cleared += 1
                        except:
                            pass
            
            # æ¸…ç†äºŒè¿›åˆ¶ç¼“å­˜
            for file_name in os.listdir(self.binary_cache_dir):
                if file_name.endswith('.pkl'):
                    file_path = os.path.join(self.binary_cache_dir, file_name)
                    if datetime.fromtimestamp(os.path.getmtime(file_path)) < cutoff_time:
                        try:
                            os.remove(file_path)
                        except:
                            pass
        else:
            # æ¸…ç†æ‰€æœ‰ç¼“å­˜æ–‡ä»¶
            for file_name in os.listdir(self.cache_dir):
                if file_name.endswith('.json'):
                    try:
                        os.remove(os.path.join(self.cache_dir, file_name))
                        cleared += 1
                    except:
                        pass
            
            for file_name in os.listdir(self.binary_cache_dir):
                if file_name.endswith('.pkl'):
                    try:
                        os.remove(os.path.join(self.binary_cache_dir, file_name))
                    except:
                        pass
        
        return cleared
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            total_requests = self.stats['hits'] + self.stats['misses']
            hit_rate = self.stats['hits'] / total_requests if total_requests > 0 else 0
            memory_hit_rate = self.stats['memory_hits'] / self.stats['hits'] if self.stats['hits'] > 0 else 0
            
            return {
                'total_requests': total_requests,
                'hits': self.stats['hits'],
                'misses': self.stats['misses'],
                'hit_rate': f"{hit_rate:.1%}",
                'memory_hits': self.stats['memory_hits'],
                'disk_hits': self.stats['disk_hits'],
                'memory_hit_rate': f"{memory_hit_rate:.1%}",
                'memory_cache_size': len(self.memory_cache),
                'memory_cache_limit': self.max_memory_items
            }

class OptimizedParameterSearch:
    """ä¼˜åŒ–å‚æ•°æœç´¢ - é«˜æ•ˆæœç´¢æœ€ä½³å‚æ•°"""
    
    def __init__(self, max_workers: int = 32):
        """
        åˆå§‹åŒ–å‚æ•°æœç´¢
        
        Args:
            max_workers: æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°
        """
        self.max_workers = max_workers
        self.cache = SmartCache("parameter_cache")
    
    def search(self, 
              parameter_space: Dict[str, List[Any]], 
              evaluation_func: Callable[[Dict[str, Any]], float],
              cache_key: Optional[str] = None) -> Dict[str, Any]:
        """
        æœç´¢æœ€ä½³å‚æ•°
        
        Args:
            parameter_space: å‚æ•°ç©ºé—´ï¼Œé”®ä¸ºå‚æ•°åï¼Œå€¼ä¸ºå¯èƒ½çš„å‚æ•°å€¼åˆ—è¡¨
            evaluation_func: è¯„ä¼°å‡½æ•°ï¼Œæ¥å—å‚æ•°å­—å…¸ï¼Œè¿”å›è¯„åˆ†ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
            cache_key: ç¼“å­˜é”®ï¼Œå¦‚æœæä¾›åˆ™å°è¯•ä½¿ç”¨ç¼“å­˜
        
        Returns:
            æœ€ä½³å‚æ•°ç»„åˆ
        """
        start_time = time.time()
        
        # å°è¯•ä»ç¼“å­˜è·å–
        if cache_key:
            cached_result = self.cache.get(cache_key)
            if cached_result:
                print(f"ğŸ“‚ ä½¿ç”¨ç¼“å­˜çš„å‚æ•°æœç´¢ç»“æœ: {cache_key}")
                return cached_result
        
        import itertools
        
        # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
        param_names = list(parameter_space.keys())
        param_values = list(parameter_space.values())
        combinations = list(itertools.product(*param_values))
        
        print(f"ğŸ” å‚æ•°æœç´¢: {len(combinations)} ç§ç»„åˆ (çº¿ç¨‹æ•°: {self.max_workers})")
        
        best_params = None
        best_score = float('-inf')
        progress = ProgressTracker(len(combinations), "å‚æ•°æœç´¢")
        
        # å†…å­˜ä¸­ä¿å­˜æœ€è¿‘çš„è¯„ä¼°ç»“æœï¼Œé¿å…é‡å¤è®¡ç®—
        evaluation_cache = {}
        eval_cache_hits = 0
        eval_cache_lock = threading.Lock()
        
        def evaluate_params_cached(params):
            """å¸¦ç¼“å­˜çš„å‚æ•°è¯„ä¼°"""
            # ç”Ÿæˆå‚æ•°çš„å“ˆå¸Œé”®
            param_str = json.dumps(params, sort_keys=True)
            param_key = hashlib.md5(param_str.encode()).hexdigest()
            
            # æ£€æŸ¥å†…å­˜ç¼“å­˜
            with eval_cache_lock:
                if param_key in evaluation_cache:
                    nonlocal eval_cache_hits
                    eval_cache_hits += 1
                    return evaluation_cache[param_key]
            
            # æ‰§è¡Œè¯„ä¼°
            try:
                score = evaluation_func(params)
                
                # ç¼“å­˜ç»“æœ
                with eval_cache_lock:
                    evaluation_cache[param_key] = score
                
                return score
            except Exception as e:
                return float('-inf')
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_params = {}
            for i, values in enumerate(combinations):
                params = dict(zip(param_names, values))
                future = executor.submit(evaluate_params_cached, params)
                future_to_params[future] = params
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_params):
                params = future_to_params[future]
                try:
                    score = future.result()
                    progress.update()
                    
                    if score > best_score:
                        best_score = score
                        best_params = params
                except Exception:
                    progress.update()
        
        # è®¡ç®—æœç´¢æ—¶é—´
        search_time = time.time() - start_time
        
        # æ˜¾ç¤ºè¯„ä¼°ç¼“å­˜å‘½ä¸­ç‡
        cache_hit_rate = eval_cache_hits / len(combinations) if combinations else 0
        print(f"ğŸ“Š è¯„ä¼°ç¼“å­˜å‘½ä¸­ç‡: {cache_hit_rate:.1%} ({eval_cache_hits}/{len(combinations)})")
        
        result = {
            'best_parameters': best_params,
            'best_score': best_score,
            'search_space_size': len(combinations),
            'search_time': search_time,
            'search_speed': len(combinations) / search_time if search_time > 0 else 0
        }
        
        print(f"âœ… å‚æ•°æœç´¢å®Œæˆ! è€—æ—¶: {search_time:.2f}ç§’, æœ€ä½³å¾—åˆ†: {best_score:.3f}")
        print(f"   æœç´¢é€Ÿåº¦: {result['search_speed']:.1f} ç»„åˆ/ç§’")
        
        # ç¼“å­˜ç»“æœ
        if cache_key:
            self.cache.set(cache_key, result)
        
        return result

# å‘åå…¼å®¹çš„åˆ«å
HighPerformanceCache = SmartCache
HighPerformanceParameterSearch = OptimizedParameterSearch

def optimize_system_for_performance():
    """ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½"""
    print("ğŸš€ æ­£åœ¨ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½...")
    
    # é…ç½®åƒåœ¾å›æ”¶
    if PERFORMANCE_CONFIG['aggressive_gc']:
        gc.enable()
        old_threshold = gc.get_threshold()
        gc.set_threshold(700, 10, 5)
        print(f"âœ… åƒåœ¾å›æ”¶å·²ä¼˜åŒ–: {old_threshold} -> (700, 10, 5)")
    
    # æ¸…ç†ä¸å¿…è¦çš„æ¨¡å—
    initial_modules = len(sys.modules)
    for module_name in list(sys.modules.keys()):
        if module_name.startswith('_') and not module_name.startswith('__'):
            try:
                del sys.modules[module_name]
            except:
                pass
    print(f"âœ… æ¸…ç†äº† {initial_modules - len(sys.modules)} ä¸ªä¸å¿…è¦çš„æ¨¡å—")
    
    # å¼ºåˆ¶åƒåœ¾å›æ”¶
    collected = gc.collect()
    print(f"âœ… å¼ºåˆ¶åƒåœ¾å›æ”¶: å›æ”¶äº† {collected} ä¸ªå¯¹è±¡")
    
    print("âœ… ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–å®Œæˆ")
    return True

if __name__ == "__main__":
    # æµ‹è¯•æ€§èƒ½ä¼˜åŒ–æ¨¡å—
    optimize_system_for_performance()
    
    # æµ‹è¯•æ‰¹é‡å¤„ç†
    processor = BatchProcessor(max_workers=8)
    print("æ‰¹é‡å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
    
    # æµ‹è¯•ç¼“å­˜
    cache = SmartCache("test_cache")
    cache.set("test_key", {"value": "test_value"})
    result = cache.get("test_key")
    print(f"ç¼“å­˜æµ‹è¯•: {result}")
    
    print("æ€§èƒ½ä¼˜åŒ–æ¨¡å—æµ‹è¯•å®Œæˆ")