#!/usr/bin/env python3
"""
å¤šå‘¨æœŸæ•°æ®ç®¡ç†å™¨
è´Ÿè´£å¤šæ—¶é—´æ¡†æ¶æ•°æ®çš„åŠ è½½ã€åŒæ­¥ã€ç¼“å­˜å’Œç®¡ç†
"""

import os
import sys
import json
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import logging
from pathlib import Path

# æ·»åŠ backendç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(__file__))

import data_loader
import indicators

class MultiTimeframeDataManager:
    """å¤šå‘¨æœŸæ•°æ®ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = "analysis_cache"):
        """åˆå§‹åŒ–å¤šå‘¨æœŸæ•°æ®ç®¡ç†å™¨"""
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # å®šä¹‰æ”¯æŒçš„æ—¶é—´å‘¨æœŸï¼ˆä½¿ç”¨æ–°çš„pandasé¢‘ç‡å­—ç¬¦ä¸²æ ¼å¼ï¼‰
        self.timeframes = {
            '5min': {'freq': '5min', 'description': '5åˆ†é’Ÿ'},
            '15min': {'freq': '15min', 'description': '15åˆ†é’Ÿ'},
            '30min': {'freq': '30min', 'description': '30åˆ†é’Ÿ'},
            '1hour': {'freq': '1h', 'description': '1å°æ—¶'},
            '4hour': {'freq': '4h', 'description': '4å°æ—¶'},
            '1day': {'freq': '1D', 'description': 'æ—¥çº¿'},
            '1week': {'freq': '1W', 'description': 'å‘¨çº¿'}
        }
        
        # å‘¨æœŸæƒé‡ï¼ˆç”¨äºä¿¡å·èåˆï¼‰
        self.timeframe_weights = {
            '1week': 0.40,   # é•¿æœŸè¶‹åŠ¿æƒé‡ï¼ˆæœ€é«˜ï¼‰
            '1day': 0.25,    # ä¸»è¶‹åŠ¿æƒé‡
            '4hour': 0.20,   # ä¸­æœŸè¶‹åŠ¿æƒé‡
            '1hour': 0.10,   # çŸ­æœŸè¶‹åŠ¿æƒé‡
            '30min': 0.03,   # å…¥åœºæ—¶æœºæƒé‡
            '15min': 0.015,  # ç²¾ç¡®å…¥åœºæƒé‡
            '5min': 0.005    # å¾®è°ƒæƒé‡
        }
        
        # æ•°æ®ç¼“å­˜
        self.data_cache = {}
        self.indicators_cache = {}
        
        # æ—¥å¿—è®¾ç½®
        self.logger = logging.getLogger(__name__)
        
    def get_synchronized_data(self, stock_code: str, timeframes: List[str] = None) -> Dict:
        """è·å–æ—¶é—´åŒæ­¥çš„å¤šå‘¨æœŸæ•°æ®"""
        if timeframes is None:
            timeframes = list(self.timeframes.keys())
        
        try:
            self.logger.info(f"è·å–{stock_code}çš„å¤šå‘¨æœŸæ•°æ®: {timeframes}")
            
            # æ£€æŸ¥ç¼“å­˜
            cache_key = f"{stock_code}_{'-'.join(timeframes)}"
            if cache_key in self.data_cache:
                cache_data = self.data_cache[cache_key]
                if self._is_cache_valid(cache_data):
                    return cache_data
            
            # è·å–åŸºç¡€æ•°æ®
            multi_data = data_loader.get_multi_timeframe_data(stock_code)
            
            if not multi_data['data_status']['daily_available'] and not multi_data['data_status']['min5_available']:
                return {'error': f'æ— æ³•è·å–{stock_code}çš„åŸºç¡€æ•°æ®'}
            
            # æ„å»ºå¤šå‘¨æœŸæ•°æ®é›†
            synchronized_data = {
                'stock_code': stock_code,
                'timeframes': {},
                'alignment_info': {},
                'data_quality': {},
                'last_update': datetime.now().isoformat()
            }
            
            # å¤„ç†æ¯ä¸ªæ—¶é—´å‘¨æœŸ
            for timeframe in timeframes:
                try:
                    tf_data = self._get_timeframe_data(multi_data, timeframe)
                    if tf_data is not None and not tf_data.empty:
                        # æ•°æ®è´¨é‡æ£€æŸ¥
                        quality_info = self._check_data_quality(tf_data, timeframe)
                        
                        synchronized_data['timeframes'][timeframe] = tf_data
                        synchronized_data['data_quality'][timeframe] = quality_info
                        
                        self.logger.info(f"  {timeframe}: {len(tf_data)} æ¡æ•°æ®")
                    else:
                        self.logger.warning(f"  {timeframe}: æ— æ•°æ®")
                        
                except Exception as e:
                    self.logger.error(f"å¤„ç†{timeframe}æ•°æ®å¤±è´¥: {e}")
                    continue
            
            # æ—¶é—´è½´å¯¹é½
            if len(synchronized_data['timeframes']) > 1:
                alignment_info = self._align_timeframes(synchronized_data['timeframes'])
                synchronized_data['alignment_info'] = alignment_info
            
            # ç¼“å­˜ç»“æœ
            self.data_cache[cache_key] = synchronized_data
            
            return synchronized_data
            
        except Exception as e:
            self.logger.error(f"è·å–{stock_code}å¤šå‘¨æœŸæ•°æ®å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _get_timeframe_data(self, multi_data: Dict, timeframe: str) -> Optional[pd.DataFrame]:
        """è·å–æŒ‡å®šæ—¶é—´å‘¨æœŸçš„æ•°æ®"""
        try:
            if timeframe == '1day':
                # æ—¥çº¿æ•°æ®
                return multi_data.get('daily_data')
            
            elif timeframe == '5min':
                # 5åˆ†é’Ÿæ•°æ®
                return multi_data.get('min5_data')
            
            elif timeframe == '1week':
                # å‘¨çº¿æ•°æ®éœ€è¦ä»æ—¥çº¿æ•°æ®é‡é‡‡æ ·
                daily_data = multi_data.get('daily_data')
                if daily_data is None or daily_data.empty:
                    return None
                return self._resample_data(daily_data, timeframe)
            
            else:
                # å…¶ä»–å‘¨æœŸéœ€è¦ä»5åˆ†é’Ÿæ•°æ®é‡é‡‡æ ·
                min5_data = multi_data.get('min5_data')
                if min5_data is None or min5_data.empty:
                    return None
                
                return self._resample_data(min5_data, timeframe)
                
        except Exception as e:
            self.logger.error(f"è·å–{timeframe}æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _resample_data(self, min5_data: pd.DataFrame, target_timeframe: str) -> pd.DataFrame:
        """å°†5åˆ†é’Ÿæ•°æ®é‡é‡‡æ ·åˆ°ç›®æ ‡æ—¶é—´å‘¨æœŸ"""
        try:
            freq = self.timeframes[target_timeframe]['freq']
            
            # ç¡®ä¿ç´¢å¼•æ˜¯DatetimeIndex
            if not isinstance(min5_data.index, pd.DatetimeIndex):
                if 'datetime' in min5_data.columns:
                    min5_data = min5_data.set_index('datetime')
                else:
                    return None
            
            # é‡é‡‡æ ·è§„åˆ™
            agg_rules = {
                'open': 'first',
                'high': 'max',
                'low': 'min',
                'close': 'last',
                'volume': 'sum'
            }
            
            # å¦‚æœæœ‰amountåˆ—ä¹Ÿå¤„ç†
            if 'amount' in min5_data.columns:
                agg_rules['amount'] = 'sum'
            
            # æ‰§è¡Œé‡é‡‡æ ·
            resampled = min5_data.resample(freq).agg(agg_rules)
            
            # ç§»é™¤ç©ºå€¼è¡Œ
            resampled = resampled.dropna()
            
            return resampled
            
        except Exception as e:
            self.logger.error(f"é‡é‡‡æ ·åˆ°{target_timeframe}å¤±è´¥: {e}")
            return None
    
    def _align_timeframes(self, timeframes_data: Dict) -> Dict:
        """å¯¹é½ä¸åŒå‘¨æœŸçš„æ—¶é—´è½´"""
        try:
            alignment_info = {
                'common_start_time': None,
                'common_end_time': None,
                'time_coverage': {},
                'data_gaps': {}
            }
            
            # æ‰¾åˆ°å…±åŒçš„æ—¶é—´èŒƒå›´
            start_times = []
            end_times = []
            
            for timeframe, df in timeframes_data.items():
                if df is not None and not df.empty:
                    # ç¡®ä¿ç´¢å¼•æ˜¯DatetimeIndex
                    if not isinstance(df.index, pd.DatetimeIndex):
                        self.logger.warning(f"{timeframe}æ•°æ®ç´¢å¼•ä¸æ˜¯DatetimeIndexï¼Œè·³è¿‡æ—¶é—´å¯¹é½")
                        continue
                    
                    start_time = df.index[0]
                    end_time = df.index[-1]
                    
                    # ç¡®ä¿æ˜¯Timestampå¯¹è±¡
                    if isinstance(start_time, pd.Timestamp) and isinstance(end_time, pd.Timestamp):
                        start_times.append(start_time)
                        end_times.append(end_time)
                        
                        alignment_info['time_coverage'][timeframe] = {
                            'start': start_time.isoformat(),
                            'end': end_time.isoformat(),
                            'count': len(df)
                        }
                    else:
                        self.logger.warning(f"{timeframe}æ•°æ®æ—¶é—´æ ¼å¼å¼‚å¸¸ï¼Œè·³è¿‡æ—¶é—´å¯¹é½")
            
            if start_times and end_times:
                try:
                    max_start = max(start_times)
                    min_end = min(end_times)
                    alignment_info['common_start_time'] = max_start.isoformat()
                    alignment_info['common_end_time'] = min_end.isoformat()
                except Exception as e:
                    self.logger.error(f"è®¡ç®—å…±åŒæ—¶é—´èŒƒå›´å¤±è´¥: {e}")
                    alignment_info['common_start_time'] = None
                    alignment_info['common_end_time'] = None
            
            # æ£€æŸ¥æ•°æ®ç¼ºå£
            for timeframe, df in timeframes_data.items():
                if df is not None and not df.empty:
                    gaps = self._detect_data_gaps(df, timeframe)
                    if gaps:
                        alignment_info['data_gaps'][timeframe] = gaps
            
            return alignment_info
            
        except Exception as e:
            self.logger.error(f"æ—¶é—´è½´å¯¹é½å¤±è´¥: {e}")
            return {}
    
    def _detect_data_gaps(self, df: pd.DataFrame, timeframe: str) -> List[Dict]:
        """æ£€æµ‹æ•°æ®ç¼ºå£"""
        try:
            gaps = []
            freq = self.timeframes[timeframe]['freq']
            
            # ç”ŸæˆæœŸæœ›çš„æ—¶é—´åºåˆ—
            expected_times = pd.date_range(
                start=df.index[0],
                end=df.index[-1],
                freq=freq
            )
            
            # æ‰¾åˆ°ç¼ºå¤±çš„æ—¶é—´ç‚¹
            missing_times = expected_times.difference(df.index)
            
            if len(missing_times) > 0:
                # å°†è¿ç»­çš„ç¼ºå¤±æ—¶é—´åˆ†ç»„
                gap_groups = []
                current_group = [missing_times[0]]
                
                for i in range(1, len(missing_times)):
                    time_diff = missing_times[i] - missing_times[i-1]
                    expected_diff = pd.Timedelta(freq)
                    
                    if time_diff <= expected_diff * 1.5:  # å…è®¸ä¸€å®šçš„è¯¯å·®
                        current_group.append(missing_times[i])
                    else:
                        gap_groups.append(current_group)
                        current_group = [missing_times[i]]
                
                gap_groups.append(current_group)
                
                # è½¬æ¢ä¸ºç¼ºå£ä¿¡æ¯
                for group in gap_groups:
                    gaps.append({
                        'start': group[0].isoformat(),
                        'end': group[-1].isoformat(),
                        'duration': len(group),
                        'timeframe': timeframe
                    })
            
            return gaps
            
        except Exception as e:
            self.logger.error(f"æ£€æµ‹{timeframe}æ•°æ®ç¼ºå£å¤±è´¥: {e}")
            return []
    
    def _check_data_quality(self, df: pd.DataFrame, timeframe: str) -> Dict:
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        try:
            quality_info = {
                'total_records': len(df),
                'null_values': df.isnull().sum().to_dict(),
                'duplicate_timestamps': df.index.duplicated().sum(),
                'price_anomalies': 0,
                'volume_anomalies': 0,
                'quality_score': 1.0
            }
            
            if len(df) == 0:
                quality_info['quality_score'] = 0.0
                return quality_info
            
            # æ£€æŸ¥ä»·æ ¼å¼‚å¸¸
            if 'close' in df.columns:
                price_changes = df['close'].pct_change().abs()
                # å•å‘¨æœŸæ¶¨è·Œå¹…è¶…è¿‡20%è§†ä¸ºå¼‚å¸¸
                price_anomalies = (price_changes > 0.20).sum()
                quality_info['price_anomalies'] = price_anomalies
            
            # æ£€æŸ¥æˆäº¤é‡å¼‚å¸¸
            if 'volume' in df.columns:
                volume_median = df['volume'].median()
                if volume_median > 0:
                    volume_ratios = df['volume'] / volume_median
                    # æˆäº¤é‡è¶…è¿‡ä¸­ä½æ•°50å€è§†ä¸ºå¼‚å¸¸
                    volume_anomalies = (volume_ratios > 50).sum()
                    quality_info['volume_anomalies'] = volume_anomalies
            
            # è®¡ç®—è´¨é‡è¯„åˆ†
            total_issues = (
                sum(quality_info['null_values'].values()) +
                quality_info['duplicate_timestamps'] +
                quality_info['price_anomalies'] +
                quality_info['volume_anomalies']
            )
            
            if len(df) > 0:
                quality_info['quality_score'] = max(0.0, 1.0 - (total_issues / len(df)))
            
            return quality_info
            
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥{timeframe}æ•°æ®è´¨é‡å¤±è´¥: {e}")
            return {'quality_score': 0.0, 'error': str(e)}
    
    def calculate_multi_timeframe_indicators(self, stock_code: str, timeframes: List[str] = None) -> Dict:
        """è®¡ç®—å¤šå‘¨æœŸæŠ€æœ¯æŒ‡æ ‡"""
        try:
            if timeframes is None:
                timeframes = list(self.timeframes.keys())
            
            # æ£€æŸ¥æŒ‡æ ‡ç¼“å­˜
            cache_key = f"{stock_code}_indicators_{'-'.join(timeframes)}"
            if cache_key in self.indicators_cache:
                cache_data = self.indicators_cache[cache_key]
                if self._is_cache_valid(cache_data):
                    return cache_data
            
            # è·å–å¤šå‘¨æœŸæ•°æ®
            sync_data = self.get_synchronized_data(stock_code, timeframes)
            if 'error' in sync_data:
                return sync_data
            
            indicators_result = {
                'stock_code': stock_code,
                'timeframes': {},
                'cross_timeframe_analysis': {},
                'last_update': datetime.now().isoformat()
            }
            
            # ä¸ºæ¯ä¸ªæ—¶é—´å‘¨æœŸè®¡ç®—æŒ‡æ ‡
            for timeframe in timeframes:
                df = sync_data['timeframes'].get(timeframe)
                if df is None or df.empty:
                    continue
                
                try:
                    tf_indicators = self._calculate_timeframe_indicators(df, timeframe)
                    indicators_result['timeframes'][timeframe] = tf_indicators
                    
                    self.logger.info(f"  {timeframe}æŒ‡æ ‡è®¡ç®—å®Œæˆ")
                    
                except Exception as e:
                    self.logger.error(f"è®¡ç®—{timeframe}æŒ‡æ ‡å¤±è´¥: {e}")
                    continue
            
            # è·¨å‘¨æœŸåˆ†æ
            if len(indicators_result['timeframes']) > 1:
                cross_analysis = self._analyze_cross_timeframe_patterns(indicators_result['timeframes'])
                indicators_result['cross_timeframe_analysis'] = cross_analysis
            
            # ç¼“å­˜ç»“æœ
            self.indicators_cache[cache_key] = indicators_result
            
            return indicators_result
            
        except Exception as e:
            self.logger.error(f"è®¡ç®—{stock_code}å¤šå‘¨æœŸæŒ‡æ ‡å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _calculate_timeframe_indicators(self, df: pd.DataFrame, timeframe: str) -> Dict:
        """è®¡ç®—å•ä¸ªæ—¶é—´å‘¨æœŸçš„æŠ€æœ¯æŒ‡æ ‡"""
        try:
            tf_indicators = {
                'timeframe': timeframe,
                'data_points': len(df),
                'indicators': {},
                'signals': {},
                'trend_analysis': {}
            }
            
            # åŸºç¡€æŠ€æœ¯æŒ‡æ ‡
            # MACD
            macd_values = indicators.calculate_macd(df)
            tf_indicators['indicators']['macd'] = {
                'dif': macd_values[0].tolist() if hasattr(macd_values[0], 'tolist') else [],
                'dea': macd_values[1].tolist() if hasattr(macd_values[1], 'tolist') else [],
                'histogram': (macd_values[0] - macd_values[1]).tolist() if len(macd_values) >= 2 else []
            }
            
            # KDJ
            kdj_values = indicators.calculate_kdj(df)
            tf_indicators['indicators']['kdj'] = {
                'k': kdj_values[0].tolist() if hasattr(kdj_values[0], 'tolist') else [],
                'd': kdj_values[1].tolist() if hasattr(kdj_values[1], 'tolist') else [],
                'j': kdj_values[2].tolist() if hasattr(kdj_values[2], 'tolist') else []
            }
            
            # RSI
            rsi_6 = indicators.calculate_rsi(df, 6)
            rsi_14 = indicators.calculate_rsi(df, 14)
            tf_indicators['indicators']['rsi'] = {
                'rsi_6': rsi_6.tolist() if hasattr(rsi_6, 'tolist') else [],
                'rsi_14': rsi_14.tolist() if hasattr(rsi_14, 'tolist') else []
            }
            
            # ç§»åŠ¨å¹³å‡çº¿
            ma_5 = df['close'].rolling(window=5).mean()
            ma_10 = df['close'].rolling(window=10).mean()
            ma_20 = df['close'].rolling(window=20).mean()
            tf_indicators['indicators']['ma'] = {
                'ma_5': ma_5.tolist() if hasattr(ma_5, 'tolist') else [],
                'ma_10': ma_10.tolist() if hasattr(ma_10, 'tolist') else [],
                'ma_20': ma_20.tolist() if hasattr(ma_20, 'tolist') else []
            }
            
            # å¸ƒæ—å¸¦
            bb_upper, bb_middle, bb_lower = indicators.calculate_bollinger_bands(df)
            tf_indicators['indicators']['bollinger'] = {
                'upper': bb_upper.tolist() if hasattr(bb_upper, 'tolist') else [],
                'middle': bb_middle.tolist() if hasattr(bb_middle, 'tolist') else [],
                'lower': bb_lower.tolist() if hasattr(bb_lower, 'tolist') else []
            }
            
            # è¶‹åŠ¿åˆ†æ
            tf_indicators['trend_analysis'] = self._analyze_trend(df, timeframe)
            
            # ä¿¡å·åˆ†æ
            tf_indicators['signals'] = self._analyze_signals(df, tf_indicators['indicators'], timeframe)
            
            return tf_indicators
            
        except Exception as e:
            self.logger.error(f"è®¡ç®—{timeframe}æŒ‡æ ‡å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _analyze_trend(self, df: pd.DataFrame, timeframe: str) -> Dict:
        """åˆ†æè¶‹åŠ¿"""
        try:
            if len(df) < 20:
                return {'trend': 'insufficient_data'}
            
            # ä»·æ ¼è¶‹åŠ¿
            recent_prices = df['close'].tail(20)
            price_trend = 'sideways'
            
            if recent_prices.iloc[-1] > recent_prices.iloc[0] * 1.05:
                price_trend = 'uptrend'
            elif recent_prices.iloc[-1] < recent_prices.iloc[0] * 0.95:
                price_trend = 'downtrend'
            
            # æˆäº¤é‡è¶‹åŠ¿
            volume_trend = 'stable'
            if 'volume' in df.columns and len(df) >= 10:
                recent_volume = df['volume'].tail(10).mean()
                historical_volume = df['volume'].head(-10).mean()
                
                if recent_volume > historical_volume * 1.2:
                    volume_trend = 'increasing'
                elif recent_volume < historical_volume * 0.8:
                    volume_trend = 'decreasing'
            
            # æ³¢åŠ¨ç‡
            returns = df['close'].pct_change().dropna()
            volatility = returns.std() * np.sqrt(252) if len(returns) > 0 else 0
            
            return {
                'price_trend': price_trend,
                'volume_trend': volume_trend,
                'volatility': volatility,
                'trend_strength': abs(recent_prices.iloc[-1] / recent_prices.iloc[0] - 1),
                'timeframe': timeframe
            }
            
        except Exception as e:
            self.logger.error(f"åˆ†æ{timeframe}è¶‹åŠ¿å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _analyze_signals(self, df: pd.DataFrame, indicators_dict: Dict, timeframe: str) -> Dict:
        """åˆ†æäº¤æ˜“ä¿¡å·"""
        try:
            signals = {
                'macd_signals': [],
                'kdj_signals': [],
                'rsi_signals': [],
                'ma_signals': [],
                'composite_signal': 'neutral'
            }
            
            if len(df) < 2:
                return signals
            
            # MACDä¿¡å·
            macd_dif = indicators_dict.get('macd', {}).get('dif', [])
            macd_dea = indicators_dict.get('macd', {}).get('dea', [])
            
            if len(macd_dif) >= 2 and len(macd_dea) >= 2:
                if macd_dif[-2] <= macd_dea[-2] and macd_dif[-1] > macd_dea[-1]:
                    signals['macd_signals'].append('golden_cross')
                elif macd_dif[-2] >= macd_dea[-2] and macd_dif[-1] < macd_dea[-1]:
                    signals['macd_signals'].append('death_cross')
            
            # KDJä¿¡å·
            kdj_k = indicators_dict.get('kdj', {}).get('k', [])
            kdj_d = indicators_dict.get('kdj', {}).get('d', [])
            
            if len(kdj_k) >= 2 and len(kdj_d) >= 2:
                if kdj_k[-2] <= kdj_d[-2] and kdj_k[-1] > kdj_d[-1] and kdj_d[-1] < 30:
                    signals['kdj_signals'].append('oversold_golden_cross')
                elif kdj_k[-2] >= kdj_d[-2] and kdj_k[-1] < kdj_d[-1] and kdj_d[-1] > 70:
                    signals['kdj_signals'].append('overbought_death_cross')
            
            # RSIä¿¡å·
            rsi_14 = indicators_dict.get('rsi', {}).get('rsi_14', [])
            if len(rsi_14) >= 1:
                if rsi_14[-1] < 30:
                    signals['rsi_signals'].append('oversold')
                elif rsi_14[-1] > 70:
                    signals['rsi_signals'].append('overbought')
            
            # ç§»åŠ¨å¹³å‡çº¿ä¿¡å·
            ma_5 = indicators_dict.get('ma', {}).get('ma_5', [])
            ma_20 = indicators_dict.get('ma', {}).get('ma_20', [])
            
            if len(ma_5) >= 2 and len(ma_20) >= 2:
                if ma_5[-2] <= ma_20[-2] and ma_5[-1] > ma_20[-1]:
                    signals['ma_signals'].append('ma5_cross_ma20_up')
                elif ma_5[-2] >= ma_20[-2] and ma_5[-1] < ma_20[-1]:
                    signals['ma_signals'].append('ma5_cross_ma20_down')
            
            # ç»¼åˆä¿¡å·
            bullish_signals = len([s for s in signals['macd_signals'] if 'golden' in s]) + \
                             len([s for s in signals['kdj_signals'] if 'golden' in s]) + \
                             len([s for s in signals['ma_signals'] if 'up' in s])
            
            bearish_signals = len([s for s in signals['macd_signals'] if 'death' in s]) + \
                            len([s for s in signals['kdj_signals'] if 'death' in s]) + \
                            len([s for s in signals['ma_signals'] if 'down' in s])
            
            if bullish_signals > bearish_signals:
                signals['composite_signal'] = 'bullish'
            elif bearish_signals > bullish_signals:
                signals['composite_signal'] = 'bearish'
            
            return signals
            
        except Exception as e:
            self.logger.error(f"åˆ†æ{timeframe}ä¿¡å·å¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _analyze_cross_timeframe_patterns(self, timeframes_indicators: Dict) -> Dict:
        """åˆ†æè·¨å‘¨æœŸæ¨¡å¼"""
        try:
            cross_analysis = {
                'trend_alignment': {},
                'signal_convergence': {},
                'strength_analysis': {},
                'recommendation': 'neutral'
            }
            
            # è¶‹åŠ¿ä¸€è‡´æ€§åˆ†æ
            trends = {}
            for timeframe, indicators in timeframes_indicators.items():
                trend_info = indicators.get('trend_analysis', {})
                trends[timeframe] = trend_info.get('price_trend', 'unknown')
            
            # è®¡ç®—è¶‹åŠ¿ä¸€è‡´æ€§
            uptrend_count = sum(1 for trend in trends.values() if trend == 'uptrend')
            downtrend_count = sum(1 for trend in trends.values() if trend == 'downtrend')
            total_timeframes = len(trends)
            
            cross_analysis['trend_alignment'] = {
                'trends': trends,
                'uptrend_ratio': uptrend_count / total_timeframes if total_timeframes > 0 else 0,
                'downtrend_ratio': downtrend_count / total_timeframes if total_timeframes > 0 else 0,
                'alignment_strength': max(uptrend_count, downtrend_count) / total_timeframes if total_timeframes > 0 else 0
            }
            
            # ä¿¡å·æ”¶æ•›åˆ†æ
            signal_convergence = {}
            for timeframe, indicators in timeframes_indicators.items():
                signals = indicators.get('signals', {})
                composite_signal = signals.get('composite_signal', 'neutral')
                signal_convergence[timeframe] = composite_signal
            
            bullish_convergence = sum(1 for signal in signal_convergence.values() if signal == 'bullish')
            bearish_convergence = sum(1 for signal in signal_convergence.values() if signal == 'bearish')
            
            cross_analysis['signal_convergence'] = {
                'signals': signal_convergence,
                'bullish_convergence': bullish_convergence / total_timeframes if total_timeframes > 0 else 0,
                'bearish_convergence': bearish_convergence / total_timeframes if total_timeframes > 0 else 0,
                'convergence_strength': max(bullish_convergence, bearish_convergence) / total_timeframes if total_timeframes > 0 else 0
            }
            
            # å¼ºåº¦åˆ†æï¼ˆåŸºäºå‘¨æœŸæƒé‡ï¼‰
            weighted_bullish_score = 0
            weighted_bearish_score = 0
            
            for timeframe, signal in signal_convergence.items():
                weight = self.timeframe_weights.get(timeframe, 0.1)
                if signal == 'bullish':
                    weighted_bullish_score += weight
                elif signal == 'bearish':
                    weighted_bearish_score += weight
            
            cross_analysis['strength_analysis'] = {
                'weighted_bullish_score': weighted_bullish_score,
                'weighted_bearish_score': weighted_bearish_score,
                'net_score': weighted_bullish_score - weighted_bearish_score
            }
            
            # ç»¼åˆå»ºè®®
            net_score = cross_analysis['strength_analysis']['net_score']
            alignment_strength = cross_analysis['trend_alignment']['alignment_strength']
            
            if net_score > 0.3 and alignment_strength > 0.6:
                cross_analysis['recommendation'] = 'strong_buy'
            elif net_score > 0.1 and alignment_strength > 0.4:
                cross_analysis['recommendation'] = 'buy'
            elif net_score < -0.3 and alignment_strength > 0.6:
                cross_analysis['recommendation'] = 'strong_sell'
            elif net_score < -0.1 and alignment_strength > 0.4:
                cross_analysis['recommendation'] = 'sell'
            else:
                cross_analysis['recommendation'] = 'neutral'
            
            return cross_analysis
            
        except Exception as e:
            self.logger.error(f"è·¨å‘¨æœŸåˆ†æå¤±è´¥: {e}")
            return {'error': str(e)}
    
    def _is_cache_valid(self, cache_data: Dict, max_age_minutes: int = 30) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        try:
            if 'last_update' not in cache_data:
                return False
            
            last_update = datetime.fromisoformat(cache_data['last_update'])
            age = datetime.now() - last_update
            
            return age.total_seconds() < max_age_minutes * 60
            
        except:
            return False
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.data_cache.clear()
        self.indicators_cache.clear()
        self.logger.info("ç¼“å­˜å·²æ¸…ç©º")
    
    def get_cache_info(self) -> Dict:
        """è·å–ç¼“å­˜ä¿¡æ¯"""
        return {
            'data_cache_size': len(self.data_cache),
            'indicators_cache_size': len(self.indicators_cache),
            'supported_timeframes': list(self.timeframes.keys()),
            'timeframe_weights': self.timeframe_weights
        }

def main():
    """æµ‹è¯•å‡½æ•°"""
    print("ğŸ• å¤šå‘¨æœŸæ•°æ®ç®¡ç†å™¨æµ‹è¯•")
    print("=" * 50)
    
    # åˆ›å»ºç®¡ç†å™¨
    manager = MultiTimeframeDataManager()
    
    # æµ‹è¯•è‚¡ç¥¨
    test_stocks = ['sz300290', 'sz002691']
    
    for stock_code in test_stocks:
        print(f"\nğŸ“Š æµ‹è¯•è‚¡ç¥¨: {stock_code}")
        
        # è·å–å¤šå‘¨æœŸæ•°æ®
        sync_data = manager.get_synchronized_data(stock_code)
        
        if 'error' in sync_data:
            print(f"  âŒ æ•°æ®è·å–å¤±è´¥: {sync_data['error']}")
            continue
        
        print(f"  âœ… è·å–åˆ° {len(sync_data['timeframes'])} ä¸ªæ—¶é—´å‘¨æœŸçš„æ•°æ®")
        
        for timeframe, df in sync_data['timeframes'].items():
            quality = sync_data['data_quality'][timeframe]
            print(f"    {timeframe}: {len(df)} æ¡æ•°æ®, è´¨é‡è¯„åˆ†: {quality['quality_score']:.2f}")
        
        # è®¡ç®—å¤šå‘¨æœŸæŒ‡æ ‡
        indicators_result = manager.calculate_multi_timeframe_indicators(stock_code)
        
        if 'error' in indicators_result:
            print(f"  âŒ æŒ‡æ ‡è®¡ç®—å¤±è´¥: {indicators_result['error']}")
            continue
        
        print(f"  âœ… è®¡ç®—äº† {len(indicators_result['timeframes'])} ä¸ªæ—¶é—´å‘¨æœŸçš„æŒ‡æ ‡")
        
        # æ˜¾ç¤ºè·¨å‘¨æœŸåˆ†æç»“æœ
        cross_analysis = indicators_result.get('cross_timeframe_analysis', {})
        if cross_analysis:
            recommendation = cross_analysis.get('recommendation', 'neutral')
            alignment_strength = cross_analysis.get('trend_alignment', {}).get('alignment_strength', 0)
            print(f"  ğŸ“ˆ è·¨å‘¨æœŸåˆ†æ: {recommendation} (è¶‹åŠ¿ä¸€è‡´æ€§: {alignment_strength:.2f})")
        
        break  # åªæµ‹è¯•ç¬¬ä¸€ä¸ªæœ‰æ•ˆè‚¡ç¥¨

if __name__ == "__main__":
    main()