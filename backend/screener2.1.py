import os
import glob
import json
import pandas as pd
import numpy as np
from multiprocessing import Pool, cpu_count
from datetime import datetime
import logging
# å‡è®¾è¿™äº›æ¨¡å—å­˜åœ¨äºæ‚¨çš„é¡¹ç›®ä¸­
# import data_loader
# import backtester
# import indicators
# from win_rate_filter import WinRateFilter, AdvancedTripleCrossFilter

# --- é…ç½® ---
BASE_PATH = os.path.expanduser("~/.local/share/tdxcfv/drive_c/tc/vipdoc")
MARKETS = ['sh', 'sz', 'bj']
# --- å·²å°†æ–°ç­–ç•¥è®¾ä¸ºé»˜è®¤ ---
STRATEGY_TO_RUN = 'ABYSS_BOTTOMING'
#STRATEGY_TO_RUN = 'MACD_ZERO_AXIS'
#STRATEGY_TO_RUN = 'TRIPLE_CROSS'

# --- è·¯å¾„å®šä¹‰ ---
backend_dir = os.path.dirname(os.path.abspath(__file__))
OUTPUT_PATH = os.path.abspath(os.path.join(backend_dir, '..', 'data', 'result'))

# --- åˆå§‹åŒ–æ—¥å¿— ---
DATE = datetime.now().strftime("%Y%m%d_%H%M")
RESULT_DIR = os.path.join(OUTPUT_PATH, STRATEGY_TO_RUN)
os.makedirs(RESULT_DIR, exist_ok=True)
LOG_FILE = os.path.join(RESULT_DIR, f'log_screener_{DATE}.txt')

file_handler = logging.FileHandler(LOG_FILE, 'a', 'utf-8')
file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
logger = logging.getLogger('screener_logger')
logger.setLevel(logging.INFO)
if logger.hasHandlers():
    logger.handlers.clear()
logger.addHandler(file_handler)


# #############################################################################
# ##### ä¿®æ­£ç‰ˆï¼šâ€œæ·±æ¸Šç­‘åº•â€ (Abyss Bottoming) ç­–ç•¥å®ç° #####
# #############################################################################

def apply_abyss_bottoming_strategy(df):
    """
    â€œæ·±æ¸Šç­‘åº•â€å››éƒ¨æ›²ç­–ç•¥ç­›é€‰å‡½æ•° (ä¿®æ­£ç‰ˆ)
    åˆ¤æ–­å½“å‰è‚¡ç¥¨æ˜¯å¦å¤„äºâ€œç¼©é‡æŒ–å‘â€åçš„ä¼ç¨³æ‹‰å‡åˆæœŸï¼ˆæœ€ä½³ä»‹å…¥ç‚¹ï¼‰

    Args:
        df (pd.DataFrame): åŒ…å«'open', 'high', 'low', 'close', 'volume'çš„æ—¥çº¿æ•°æ®

    Returns:
        tuple: (pd.Series or None, dict or None)
               å¦‚æœç¬¦åˆä¿¡å·ï¼Œè¿”å› (ä¿¡å·Series, ä¿¡å·è¯¦æƒ…å­—å…¸)ï¼Œå¦åˆ™è¿”å› (None, None)
    """
    signal_series = pd.Series(index=df.index, dtype=object).fillna('')
    
    # --- å‚æ•°å®šä¹‰ ---
    LONG_TERM_DAYS = 250 * 2
    MIN_DROP_PERCENT = 0.60
    PRICE_LOW_PERCENTILE = 0.20 # æ”¾å®½åˆ°20%
    VOLUME_LOW_PERCENTILE = 0.15 # æ”¾å®½åˆ°15%

    HIBERNATION_DAYS = 60
    HIBERNATION_VOLATILITY_MAX = 0.35 # æ”¾å®½åˆ°35%

    WASHOUT_DAYS = 30
    WASHOUT_VOLUME_SHRINK_RATIO = 0.85 # æŒ–å‘æˆäº¤é‡å°äºæ¨ªç›˜æœŸçš„85%
    
    # ç¬¬ä¸‰é˜¶æ®µï¼ˆæ‹‰å‡ï¼‰çš„åˆ¤æ–­å‚æ•°
    MAX_RISE_FROM_BOTTOM = 0.15 # ä»å‘åº•æœ€å¤§åå¼¹å¹…åº¦ä¸è¶…è¿‡15%
    LIFTOFF_VOLUME_INCREASE_RATIO = 1.2 # å¯åŠ¨æ—¥æˆäº¤é‡è‡³å°‘æ˜¯å‘å†…å‡é‡çš„1.2å€

    try:
        if len(df) < LONG_TERM_DAYS:
            return None, None

        # --- ç¬¬é›¶é˜¶æ®µï¼šæ·±è·Œç­‘åº• (The Great Premise) ---
        df_long_term = df.tail(LONG_TERM_DAYS)
        long_term_high = df_long_term['high'].max()
        long_term_low = df_long_term['low'].min()
        current_price = df['close'].iloc[-1]

        price_range = long_term_high - long_term_low
        if price_range == 0 or current_price > long_term_low + price_range * PRICE_LOW_PERCENTILE:
            return None, None
        if current_price > long_term_high * (1 - MIN_DROP_PERCENT):
            return None, None
        recent_avg_volume = df['volume'].tail(20).mean()
        volume_low_threshold = df_long_term['volume'].quantile(VOLUME_LOW_PERCENTILE)
        if recent_avg_volume > volume_low_threshold:
            return None, None

        # --- ç¬¬ä¸€é˜¶æ®µ & ç¬¬äºŒé˜¶æ®µï¼šå¯»æ‰¾ æ¨ªç›˜ + æŒ–å‘ ç»“æ„ ---
        df_washout = df.tail(WASHOUT_DAYS)
        df_hibernation = df.iloc[-(WASHOUT_DAYS + HIBERNATION_DAYS):-WASHOUT_DAYS]

        if df_hibernation.empty:
            return None, None

        hibernation_support = df_hibernation['low'].min()
        hibernation_resistance = df_hibernation['high'].max()
        
        volatility = (hibernation_resistance - hibernation_support) / hibernation_support
        if volatility > HIBERNATION_VOLATILITY_MAX or volatility <= 0:
            return None, None
        
        hibernation_avg_volume = df_hibernation['volume'].mean()
        if hibernation_avg_volume == 0:
            return None, None

        washout_low = df_washout['low'].min()
        if washout_low >= hibernation_support:
            return None, None

        pit_days_volume = df_washout[df_washout['low'] < hibernation_support]['volume']
        if pit_days_volume.empty:
            return None, None
        
        washout_avg_volume = pit_days_volume.mean()
        if washout_avg_volume == 0:
            return None, None
            
        if washout_avg_volume >= hibernation_avg_volume * WASHOUT_VOLUME_SHRINK_RATIO:
            return None, None

        # --- ç¬¬ä¸‰é˜¶æ®µï¼šç¡®è®¤æ‹‰å‡ (The Liftoff) - æ ¸å¿ƒé€»è¾‘ä¿®æ­£ ---
        last_row = df.iloc[-1]
        prev_row = df.iloc[-2]
        
        # æ¡ä»¶1: å½“å¤©å¿…é¡»æ˜¯é˜³çº¿
        is_reversal_day = last_row['close'] > last_row['open'] and last_row['close'] > prev_row['close']
        
        # æ¡ä»¶2: è‚¡ä»·å°šæœªè¿œç¦»å‘åº• (é˜²æ­¢è¿½é«˜)
        is_near_bottom = last_row['close'] < washout_low * (1 + MAX_RISE_FROM_BOTTOM)
        
        # æ¡ä»¶3: æˆäº¤é‡å¿…é¡»æ¸©å’Œæ”¾å¤§ (ç¡®è®¤èµ„é‡‘å…¥åœº)
        is_volume_recovering = last_row['volume'] > washout_avg_volume * LIFTOFF_VOLUME_INCREASE_RATIO

        # æœ€ç»ˆåˆ¤æ–­ï¼šä¸‰ä¸ªæ¡ä»¶å¿…é¡»åŒæ—¶æ»¡è¶³
        if is_reversal_day and is_near_bottom and is_volume_recovering:
            signal_series.iloc[-1] = 'BUY'
            
            # å‡†å¤‡è¯¦ç»†ä¿¡æ¯ä»¥ä¾›è¾“å‡º
            details = {
                'signal_state': 'BUY-LIFTOFF',
                'hibernation_support': f"{hibernation_support:.2f}",
                'washout_low': f"{washout_low:.2f}",
                'rise_from_low': f"{(last_row['close'] / washout_low - 1):.2%}",
                'volume_ratio': f"{(last_row['volume'] / washout_avg_volume):.2f}x"
            }
            return signal_series, details
        
        return None, None

    except Exception:
        return None, None


# ä¸ºäº†è®©è„šæœ¬å¯ä»¥ç‹¬ç«‹è¿è¡Œï¼Œæˆ‘ä»¬å°†å…¶ä»–ç­–ç•¥å‡½æ•°åˆ›å»ºä¸ºè™šæ‹Ÿå‡½æ•°
# åœ¨æ‚¨çš„å®é™…ä½¿ç”¨ä¸­ï¼Œè¯·ç¡®ä¿ strategies æ¨¡å—å’Œå…¶ä¸­çš„å‡½æ•°æ˜¯çœŸå®å­˜åœ¨çš„
class StrategyHolder: pass
strategies = StrategyHolder()
def dummy_strategy(df):
    return pd.Series([False] * len(df), index=df.index)

strategies.apply_abyss_bottoming_strategy = apply_abyss_bottoming_strategy
strategies.apply_pre_cross = dummy_strategy
strategies.apply_triple_cross = dummy_strategy
strategies.apply_macd_zero_axis_strategy = dummy_strategy
strategies.apply_weekly_golden_cross_ma_strategy = dummy_strategy

# åŒæ ·ï¼Œä¸ºå…¶ä»–ä¾èµ–åˆ›å»ºè™šæ‹Ÿå¯¹è±¡
class DummyModule:
    def __getattr__(self, name):
        def _dummy_func(*args, **kwargs): return None
        return _dummy_func
class DummyFilter: pass
data_loader = DummyModule()
backtester = DummyModule()
indicators = DummyModule()
WinRateFilter = DummyFilter
AdvancedTripleCrossFilter = DummyFilter


# #############################################################################
# #####               åŸæœ‰è„šæœ¬ä¸»è¦é€»è¾‘ï¼ˆå·²é€‚é…æ–°ç­–ç•¥ï¼‰               #####
# #############################################################################

def worker(args):
    """å¤šè¿›ç¨‹å·¥ä½œå‡½æ•°"""
    file_path, market = args
    stock_code_full = os.path.basename(file_path).split('.')[0]
    stock_code_no_prefix = stock_code_full.replace(market, '')

    valid_prefixes = ('600', '601', '603', '000', '001', '002', '003', '300', '688')
    if not stock_code_no_prefix.startswith(valid_prefixes):
        return None

    try:
        # å‡è®¾ data_loader.get_daily_data æ˜¯æ‚¨é¡¹ç›®ä¸­çœŸå®å­˜åœ¨çš„å‡½æ•°
        df = data_loader.get_daily_data(file_path)
        if df is None or len(df) < 250 * 2: # ç­–ç•¥éœ€è¦è‡³å°‘2å¹´æ•°æ®
            return None

        current_timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        latest_date = df.index[-1].strftime('%Y-%m-%d')
        
        result_base = {
            'stock_code': stock_code_full,
            'strategy': STRATEGY_TO_RUN,
            'date': latest_date,
            'scan_timestamp': current_timestamp
        }
        
        # æ ¹æ®ç­–ç•¥åˆ†å‘
        if STRATEGY_TO_RUN == 'ABYSS_BOTTOMING':
            return _process_abyss_bottoming_strategy(df, result_base)
        # ... (å…¶ä»–ç­–ç•¥çš„å¤„ç†å‡½æ•°)
        
        return None
        
    except Exception as e:
        # logger.error(f"å¤„ç† {stock_code_full} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return None

def _process_abyss_bottoming_strategy(df, result_base):
    """å¤„ç†â€œæ·±æ¸Šç­‘åº•â€ç­–ç•¥"""
    try:
        signal_series, details = strategies.apply_abyss_bottoming_strategy(df)
        
        # æ£€æŸ¥æœ€æ–°ä¸€å¤©æ˜¯å¦æœ‰'BUY'ä¿¡å·
        if signal_series is not None and details is not None and signal_series.iloc[-1] == 'BUY':
            # å°†ç­–ç•¥è¿”å›çš„è¯¦ç»†ä¿¡æ¯åˆå¹¶åˆ°ç»“æœä¸­
            result_base.update(details)
            # å¯¹äºè¿™ç§ä¿¡å·ç¨€å°‘çš„ç­–ç•¥ï¼Œå›æµ‹æ„ä¹‰ä¸å¤§ï¼Œç›´æ¥æŠ¥å‘Šå‘ç°
            result_base.update({
                'total_signals': 1,
                'win_rate': 'N/A',
                'avg_max_profit': 'N/A',
            })
            return result_base
        return None
    except Exception as e:
        logger.error(f"å¤„ç†æ·±æ¸Šç­‘åº•ç­–ç•¥å¤±è´¥ {result_base.get('stock_code', '')}: {e}")
        return None

def generate_summary_report(passed_stocks):
    """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
    if not passed_stocks:
        return {
            'scan_summary': {'total_signals': 0, 'strategy': STRATEGY_TO_RUN},
            'stocks_found': []
        }
    
    summary = {
        'scan_summary': {
            'total_signals': len(passed_stocks),
            'scan_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'strategy': STRATEGY_TO_RUN,
        },
        'stocks_found': passed_stocks # ç›´æ¥åˆ—å‡ºæ‰€æœ‰æ‰¾åˆ°çš„è‚¡ç¥¨åŠå…¶è¯¦æƒ…
    }
    return summary


def main():
    start_time = datetime.now()
    logger.info(f"===== å¼€å§‹æ‰§è¡Œæ‰¹é‡ç­›é€‰, ç­–ç•¥: {STRATEGY_TO_RUN} =====")
    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ‰¹é‡ç­›é€‰, ç­–ç•¥: {STRATEGY_TO_RUN}")
    print(f"â° æ‰«ææ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    all_files = []
    for market in MARKETS:
        path = os.path.join(BASE_PATH, market, 'lday', '*.day')
        files = glob.glob(path)
        if not files:
            print(f"âš ï¸ è­¦å‘Š: åœ¨è·¯å¾„ {path} æœªæ‰¾åˆ°ä»»ä½•æ–‡ä»¶ã€‚")
        all_files.extend([(f, market) for f in files])
    
    if not all_files:
        print("âŒ é”™è¯¯: æœªèƒ½åœ¨ä»»ä½•å¸‚åœºç›®å½•ä¸‹æ‰¾åˆ°æ—¥çº¿æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥BASE_PATHé…ç½®ã€‚")
        return

    print(f"ğŸ“Š å…±æ‰¾åˆ° {len(all_files)} ä¸ªæ—¥çº¿æ–‡ä»¶ï¼Œå¼€å§‹å¤šè¿›ç¨‹å¤„ç†...")
    
    # ä½¿ç”¨å¤šè¿›ç¨‹è¿›è¡Œåˆæ­¥ç­›é€‰
    # æ³¨æ„ï¼šWindowsä¸‹å¤šè¿›ç¨‹è°ƒè¯•å¯èƒ½ä¸ä¾¿ï¼Œå¯ä»¥å…ˆç”¨ map ä»£æ›¿ pool.map è¿›è¡Œå•è¿›ç¨‹æµ‹è¯•
    # results = list(map(worker, all_files))
    with Pool(processes=cpu_count()) as pool:
        results = pool.map(worker, all_files)
    
    passed_stocks = [r for r in results if r is not None]
    end_time = datetime.now()
    processing_time = (end_time - start_time).total_seconds()
    
    print(f"ğŸ“ˆ åˆæ­¥ç­›é€‰å®Œæˆï¼Œé€šè¿‡ç­›é€‰: {len(passed_stocks)} åªè‚¡ç¥¨")
    
    # ä¿å­˜è¯¦ç»†ä¿¡å·åˆ—è¡¨
    output_file = os.path.join(RESULT_DIR, 'signals_summary.json')
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(passed_stocks, f, ensure_ascii=False, indent=4)
    
    # ç”Ÿæˆå¹¶ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    summary_report = generate_summary_report(passed_stocks)
    summary_report['scan_summary']['processing_time'] = f"{processing_time:.2f} ç§’"
    summary_report['scan_summary']['files_processed'] = len(all_files)
    
    summary_file = os.path.join(RESULT_DIR, 'scan_summary_report.json')
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary_report, f, ensure_ascii=False, indent=4)
    
    # ç”Ÿæˆæ–‡æœ¬æ ¼å¼çš„æ±‡æ€»æŠ¥å‘Š
    text_report_file = os.path.join(RESULT_DIR, f'scan_report_{DATE}.txt')
    with open(text_report_file, 'w', encoding='utf-8') as f:
        f.write(f"=== {STRATEGY_TO_RUN} ç­–ç•¥ç­›é€‰æŠ¥å‘Š ===\n")
        f.write(f"æ‰«ææ—¶é—´: {summary_report['scan_summary']['scan_timestamp']}\n")
        f.write(f"å¤„ç†æ–‡ä»¶æ•°: {summary_report['scan_summary']['files_processed']}\n")
        f.write(f"å¤„ç†è€—æ—¶: {summary_report['scan_summary']['processing_time']}\n")
        f.write(f"å‘ç°ä¿¡å·æ•°: {summary_report['scan_summary']['total_signals']}\n\n")
        
        if passed_stocks:
            f.write("=== å‘ç°ä¿¡å·çš„è‚¡ç¥¨åˆ—è¡¨ ===\n")
            for i, stock in enumerate(passed_stocks, 1):
                details_str = ", ".join([f"{k}: {v}" for k, v in stock.items() if k not in ['stock_code', 'strategy', 'date', 'scan_timestamp']])
                f.write(f"{i:2d}. {stock['stock_code']} - {details_str}\n")

    print(f"\nğŸ“Š åˆæ­¥ç­›é€‰å®Œæˆï¼")
    print(f"ğŸ¯ å‘ç°ä¿¡å·: {len(passed_stocks)} ä¸ª")
    print(f"â±ï¸ å¤„ç†è€—æ—¶: {processing_time:.2f} ç§’")
    print(f"ğŸ“„ ç»“æœå·²ä¿å­˜è‡³:")
    print(f"  - è¯¦ç»†åˆ—è¡¨ (JSON): {output_file}")
    print(f"  - æ±‡æ€»æŠ¥å‘Š (JSON): {summary_file}")
    print(f"  - æ–‡æœ¬æŠ¥å‘Š (TXT): {text_report_file}")
    
    total_time = (datetime.now() - start_time).total_seconds()
    print(f"\nğŸ‰ å®Œæ•´æ‰«ææµç¨‹ç»“æŸï¼æ€»è€—æ—¶: {total_time:.2f} ç§’")
    logger.info(f"===== å®Œæ•´æ‰«æå®Œæˆï¼åˆæ­¥ç­›é€‰: {len(passed_stocks)} ä¸ªä¿¡å·ï¼Œæ€»è€—æ—¶: {total_time:.2f} ç§’ =====")

if __name__ == '__main__':
    # ä¸ºäº†è®©è„šæœ¬å¯ä»¥ç‹¬ç«‹è¿è¡Œï¼Œæ‚¨éœ€è¦æä¾›çœŸå®çš„ data_loader æ¨¡å—
    # ä»¥ä¸‹æ˜¯ä¸€ä¸ªè™šæ‹Ÿçš„ data_loader ç”¨äºæ¼”ç¤ºï¼Œå®ƒä¼šåˆ›å»ºä¸€ä¸ªéšæœºçš„DataFrame
    # è¯·åŠ¡å¿…æ›¿æ¢ä¸ºæ‚¨è‡ªå·±çš„çœŸå®æ•°æ®åŠ è½½å‡½æ•°
    class MockDataLoader:
        def get_daily_data(self, file_path):
            # åˆ›å»ºä¸€ä¸ªç¬¦åˆç­–ç•¥æµ‹è¯•æ¡ä»¶çš„æ¨¡æ‹ŸDataFrame
            dates = pd.to_datetime(pd.date_range(end=datetime.now(), periods=250 * 3, freq='D'))
            n = len(dates)
            
            # æ¨¡æ‹Ÿä¸€ä¸ªæ·±è·Œè¿‡ç¨‹
            high_price = 100
            low_price = 10
            prices = np.linspace(high_price, low_price, n)
            
            # å¢åŠ ä¸€äº›å™ªå£°
            noise = np.random.normal(0, 2, n).cumsum()
            prices += noise
            prices = np.maximum(prices, 1) # ä»·æ ¼ä¸ä¸ºè´Ÿ
            
            # åˆ›å»ºæ¨ªç›˜æœŸ
            hibernation_start = n - 90
            hibernation_end = n - 30
            prices[hibernation_start:hibernation_end] = np.random.uniform(low_price, low_price * 1.15, 60)
            
            # åˆ›å»ºæŒ–å‘
            pit_end = n-1
            prices[hibernation_end:pit_end] = np.random.uniform(low_price * 0.9, low_price, 29)
            
            # åˆ›å»ºæœ€åä¸€æ—¥çš„æ‹‰å‡
            prices[-1] = prices[-2] * 1.03 # æœ€åä¸€å¤©ä¸Šæ¶¨3%

            # æ¨¡æ‹Ÿæˆäº¤é‡
            volumes = np.random.randint(500000, 2000000, n)
            # åœ°é‡ç»“æ„
            volumes[int(n/2):] = np.random.randint(100000, 500000, len(volumes) - int(n/2))
            # æŒ–å‘ç¼©é‡
            volumes[hibernation_end:pit_end] = np.random.randint(50000, 100000, 29)
            # æ‹‰å‡æ”¾é‡
            volumes[-1] = volumes[-5] * 1.5


            df = pd.DataFrame({
                'open': prices * 0.99,
                'high': prices * 1.02,
                'low': prices * 0.98,
                'close': prices,
                'volume': volumes
            }, index=dates)
            
            # ä¸ºäº†è®©æ¨¡æ‹Ÿæ•°æ®ç¬¦åˆâ€œæ·±è·Œâ€æ¡ä»¶
            df.loc[df.index[0], 'high'] = high_price * 1.5 
            df.loc[df.index[n-100], 'low'] = low_price * 0.9
            return df
            
    data_loader = MockDataLoader()
    
    main()