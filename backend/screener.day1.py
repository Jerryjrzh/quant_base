import os
import glob
import json
import pandas as pd
import numpy as np
from multiprocessing import Pool, cpu_count
from datetime import datetime
import logging
# å‡è®¾è¿™äº›æ¨¡å—åœ¨æ‚¨çš„é¡¹ç›®ä¸­å­˜åœ¨
# import data_loader
# import strategies
# import backtester
# import indicators
# from win_rate_filter import WinRateFilter, AdvancedTripleCrossFilter
import talib 

# === æ¨¡æ‹Ÿæ¨¡å— (ä¸ºäº†è®©è„šæœ¬å¯ä»¥ç‹¬ç«‹è¿è¡Œ) ===
# åœ¨æ‚¨çš„çœŸå®ç¯å¢ƒä¸­ï¼Œè¯·åˆ é™¤æˆ–æ³¨é‡Šæ‰è¿™éƒ¨åˆ†ï¼Œå¹¶ç¡®ä¿æ‚¨çš„æ¨¡å—å¯è¢«å¯¼å…¥
class DummyModule:
    def __getattr__(self, name):
        if name == 'get_daily_data':
            def get_mock_data(file_path):
                dates = pd.to_datetime(pd.date_range(end=datetime.now(), periods=500, freq='D'))
                prices = pd.Series(10 + np.random.randn(500).cumsum() + np.sin(np.linspace(0, 20, 500))*2, index=dates)
                df = pd.DataFrame({'open': prices, 'high': prices*1.02, 'low': prices*0.98, 'close': prices, 'volume': np.random.randint(1e6, 5e7, 500)}, index=dates)
                return df
            return get_mock_data
        if name == 'run_backtest':
            return lambda df, signals: {'total_signals': signals.sum(), 'win_rate': '50.0%', 'avg_max_profit': '5.0%'}
        def _dummy(*args, **kwargs): return (pd.Series(), pd.Series(), pd.Series())
        return _dummy
data_loader = DummyModule()
strategies = DummyModule()
backtester = DummyModule()
indicators = DummyModule()
class WinRateFilter: pass
class AdvancedTripleCrossFilter: pass
# === æ¨¡æ‹Ÿæ¨¡å—ç»“æŸ ===


# === æ–°å¢ DAILY_MA_PULLBACK ç­–ç•¥ (V2 - å¼ºåŒ–ç‰ˆ) START ===
def resample_to_weekly(df):
    """å°†æ—¥çº¿æ•°æ®èšåˆä¸ºå‘¨çº¿æ•°æ®"""
    if df is None or df.empty: return None
    agg_dict = {'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last', 'volume': 'sum'}
    weekly_df = df.resample('W-FRI').agg(agg_dict)
    weekly_df.dropna(inplace=True)
    return weekly_df

def apply_daily_ma_pullback_strategy(df):
    """
    å‘¨çº¿ç¡®è®¤å¼ºåŠ¿ V2 (æ›´ä¸¥æ ¼)ï¼Œæ—¥çº¿å¯»æ‰¾MA13å›è¸©ä¹°ç‚¹ç­–ç•¥
    """
    if len(df) < 250: return None

    try:
        # 1. æ•°æ®å‡†å¤‡å’ŒæŒ‡æ ‡è®¡ç®—
        df_weekly = resample_to_weekly(df)
        if df_weekly is None or len(df_weekly) < 120: # éœ€è¦æ›´é•¿æ•°æ®è®¡ç®—å‘¨çº¿MA60å’Œé‡æ¯”
            return None

        # è®¡ç®—å‘¨çº¿çº§åˆ«æŒ‡æ ‡
        for p in [13, 30, 60]: df_weekly[f'ma{p}'] = talib.MA(df_weekly['close'], timeperiod=p)
        for p in [20, 60]: df_weekly[f'vol_ma{p}'] = talib.MA(df_weekly['volume'], timeperiod=p)
        # è®¡ç®—æ—¥çº¿çº§åˆ«æŒ‡æ ‡
        df[f'ma13'] = talib.MA(df['close'], timeperiod=13)

        if df_weekly.iloc[-5:].isnull().values.any() or df.iloc[-1].isnull().values.any():
            return None 

        # 2. ã€V2ç‰ˆã€‘é•¿å‘¨æœŸï¼ˆå‘¨çº¿ï¼‰ç¡®è®¤å¼ºåŠ¿è¶‹åŠ¿ (æ–°å¢ä¸¥æ ¼è¿‡æ»¤)
        latest_w = df_weekly.iloc[-1]
        
        # æ¡ä»¶a: å‘¨çº¿å‡çº¿å¤šå¤´æ’åˆ— (åŸºç¡€)
        ma_alignment_ok = latest_w['ma13'] > latest_w['ma30'] > latest_w['ma60']
        
        # ã€æ–°å¢ã€‘æ¡ä»¶b: å‡çº¿æ–œç‡æŒç»­ä¸ºæ­£ (è¦æ±‚è¶‹åŠ¿æŒç»­æ€§)
        ma30_slope_ok = latest_w['ma30'] > df_weekly['ma30'].iloc[-4] # æ£€æŸ¥MA30åœ¨è¿‡å»4å‘¨æ˜¯ä¸Šå‡çš„

        # ã€æ–°å¢ã€‘æ¡ä»¶c: å‡çº¿ä¹‹é—´æœ‰è¶³å¤Ÿå‘æ•£åº¦ (è¿‡æ»¤ç²˜åˆçŠ¶æ€)
        ma_separation_ok = (latest_w['ma13'] / latest_w['ma60'] - 1) > 0.03 # è¦æ±‚MA13è‡³å°‘é«˜äºMA60 3%

        # ã€æ–°å¢ã€‘æ¡ä»¶d: æˆäº¤é‡è¶‹åŠ¿å¥åº· (è¿‘æœŸæ”¾é‡)
        volume_trend_ok = latest_w['vol_ma20'] > latest_w['vol_ma60']

        # æœ€ç»ˆå¼ºåŠ¿åˆ¤æ–­ï¼šå¿…é¡»åŒæ—¶æ»¡è¶³æ‰€æœ‰ä¸¥æ ¼æ¡ä»¶
        is_strong_trend_weekly = ma_alignment_ok and ma30_slope_ok and ma_separation_ok and volume_trend_ok
        
        if not is_strong_trend_weekly:
            return None

        # 3. çŸ­å‘¨æœŸï¼ˆæ—¥çº¿ï¼‰ç¡®è®¤å›è¸©ä¹°ç‚¹
        latest_d = df.iloc[-1]
        
        is_pullback = latest_d['low'] <= latest_d['ma13'] * 1.01
        is_bounce = latest_d['close'] > latest_d['ma13']
        
        if is_pullback and is_bounce:
            signal_series = pd.Series(False, index=df.index)
            signal_series.iloc[-1] = True
            return signal_series

        return None
    except Exception as e:
        return None

strategies.apply_daily_ma_pullback_strategy = apply_daily_ma_pullback_strategy
# === æ–°å¢ DAILY_MA_PULLBACK ç­–ç•¥ (V2 - å¼ºåŒ–ç‰ˆ) END ===


# --- é…ç½® ---
BASE_PATH = os.path.expanduser("~/.local/share/tdxcfv/drive_c/tc/vipdoc")
MARKETS = ['sh', 'sz', 'bj']
STRATEGY_TO_RUN = 'DAILY_MA_PULLBACK'
# --- è·¯å¾„å®šä¹‰ ---
backend_dir = os.path.dirname(os.path.abspath(__file__))
OUTPUT_PATH = os.path.abspath(os.path.join(backend_dir, '..', 'data', 'result'))

# --- åˆå§‹åŒ–æ—¥å¿— ---
DATE = datetime.now().strftime("%Y%m%d_%H%M")
RESULT_DIR = os.path.join(OUTPUT_PATH, STRATEGY_TO_RUN)
os.makedirs(RESULT_DIR, exist_ok=True)
LOG_FILE = os.path.join(RESULT_DIR, f'log_screener_{DATE}.txt')

file_handler = logging.FileHandler(LOG_FILE, 'a', 'utf-8')
file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
logger = logging.getLogger('screener_logger')
logger.setLevel(logging.INFO)
if logger.hasHandlers(): logger.handlers.clear()
logger.addHandler(file_handler)

# --- åŸæœ‰è„šæœ¬å‡½æ•° (ä¿æŒä¸å˜, æ­¤å¤„ä»…ä¿ç•™å¿…è¦éƒ¨åˆ†ä»¥ä¾›è°ƒç”¨) ---
def calculate_backtest_stats_fast(df, signal_series):
    try:
        backtest_results = backtester.run_backtest(df, signal_series)
        if isinstance(backtest_results, dict) and backtest_results.get('total_signals', 0) > 0:
            return {
                'total_signals': backtest_results.get('total_signals', 0),
                'win_rate': backtest_results.get('win_rate', '0.0%'),
                'avg_max_profit': backtest_results.get('avg_max_profit', '0.0%'),
            }
        return {'total_signals': 0, 'win_rate': '0.0%', 'avg_max_profit': '0.0%'}
    except Exception as e:
        logger.error(f"å¿«é€Ÿå›æµ‹è®¡ç®—å¤±è´¥: {e}")
        return {'total_signals': 0, 'win_rate': '0.0%', 'avg_max_profit': '0.0%'}

def _process_daily_ma_pullback_strategy(df, result_base):
    """å¤„ç†DAILY_MA_PULLBACKç­–ç•¥"""
    try:
        signal_series = strategies.apply_daily_ma_pullback_strategy(df)
        if signal_series is not None and signal_series.iloc[-1]:
            backtest_stats = calculate_backtest_stats_fast(df, signal_series)
            result_base.update({'filter_status': 'passed', **backtest_stats})
            return result_base
        return None
    except Exception as e:
        logger.error(f"å¤„ç† DAILY_MA_PULLBACK ç­–ç•¥å¤±è´¥ {result_base.get('stock_code', '')}: {e}")
        return None

def worker(args):
    """å¤šè¿›ç¨‹å·¥ä½œå‡½æ•°"""
    file_path, market = args
    stock_code_full = os.path.basename(file_path).split('.')[0]
    stock_code_no_prefix = stock_code_full.replace(market, '')
    valid_prefixes = ('600', '601', '603', '000', '001', '002', '003', '300', '688')
    if not stock_code_no_prefix.startswith(valid_prefixes): return None
    try:
        df = data_loader.get_daily_data(file_path)
        if df is None or len(df) < 150: return None
        result_base = {
            'stock_code': stock_code_full, 'strategy': STRATEGY_TO_RUN,
            'date': df.index[-1].strftime('%Y-%m-%d'),
            'scan_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        if STRATEGY_TO_RUN == 'DAILY_MA_PULLBACK':
            return _process_daily_ma_pullback_strategy(df, result_base)
        # (æ­¤å¤„çœç•¥å…¶ä»–ç­–ç•¥çš„å¤„ç†é€»è¾‘)
        return None
    except Exception as e:
        logger.error(f"å¤„ç† {stock_code_full} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return None

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    start_time = datetime.now()
    logger.info(f"===== å¼€å§‹æ‰§è¡Œæ‰¹é‡ç­›é€‰, ç­–ç•¥: {STRATEGY_TO_RUN} (V2 å¼ºåŠ¿å¢å¼ºç‰ˆ) =====")
    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ‰¹é‡ç­›é€‰, ç­–ç•¥: {STRATEGY_TO_RUN} (V2 å¼ºåŠ¿å¢å¼ºç‰ˆ)")
    
    all_files = []
    for market in MARKETS:
        path = os.path.join(BASE_PATH, market, 'lday', '*.day')
        files = glob.glob(path)
        all_files.extend([(f, market) for f in files])
    
    if not all_files:
        print("âŒ é”™è¯¯: æœªèƒ½åœ¨ä»»ä½•å¸‚åœºç›®å½•ä¸‹æ‰¾åˆ°æ—¥çº¿æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥BASE_PATHé…ç½®ã€‚")
        return

    print(f"ğŸ“Š å…±æ‰¾åˆ° {len(all_files)} ä¸ªæ—¥çº¿æ–‡ä»¶ï¼Œå¼€å§‹å¤šè¿›ç¨‹å¤„ç†...")
    
    with Pool(processes=cpu_count()) as pool:
        results = pool.map(worker, all_files)
    
    passed_stocks = [r for r in results if r is not None]
    processing_time = (datetime.now() - start_time).total_seconds()
    
    print(f"ğŸ“ˆ åˆæ­¥ç­›é€‰å®Œæˆï¼Œé€šè¿‡ç­›é€‰: {len(passed_stocks)} åªè‚¡ç¥¨")
    
    output_file = os.path.join(RESULT_DIR, 'signals_summary.json')
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(passed_stocks, f, ensure_ascii=False, indent=4)

    print(f"\nğŸ“Š åˆæ­¥ç­›é€‰å®Œæˆï¼")
    print(f"ğŸ¯ å‘ç°ä¿¡å·: {len(passed_stocks)} ä¸ª")
    print(f"â±ï¸ å¤„ç†è€—æ—¶: {processing_time:.2f} ç§’")
    print(f"ğŸ“„ ç»“æœå·²ä¿å­˜è‡³: {output_file}")
    
    total_time = (datetime.now() - start_time).total_seconds()
    print(f"\nğŸ‰ å®Œæ•´æ‰«ææµç¨‹ç»“æŸï¼æ€»è€—æ—¶: {total_time:.2f} ç§’")
    logger.info(f"===== å®Œæ•´æ‰«æå®Œæˆï¼åˆæ­¥ç­›é€‰: {len(passed_stocks)} ä¸ªä¿¡å·ï¼Œæ€»è€—æ—¶: {total_time:.2f} ç§’ =====")

if __name__ == '__main__':
    main()